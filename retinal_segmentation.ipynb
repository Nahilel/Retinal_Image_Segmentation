{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoVcV-1LwcFx",
        "outputId": "086865bd-43ce-4535-b793-111cba6915ba"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYUEy8jKwNuh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from typing import Dict, Tuple, Optional, List\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.transforms import InterpolationMode\n",
        "\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwDP08_ZwNuh"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHI2hxv2wNuj"
      },
      "outputs": [],
      "source": [
        "def apply_clahe(pil_img: Image.Image) -> Image.Image:\n",
        "    \"\"\"\n",
        "    Explication de CLAHE :\n",
        "    CLAHE découpe l’image en petites régions, améliore le contraste localement dans chacune, mais limite ce contraste pour ne pas exploser le bruit, puis recolle le tout de façon lisse.\n",
        "    Applique un CLAHE couleur sur une image RGB.\n",
        "    Args:\n",
        "        pil_img: image PIL en mode 'RGB'.\n",
        "\n",
        "    Returns:\n",
        "        Image PIL après CLAHE.\n",
        "    \"\"\"\n",
        "    # Pour le traitement openCV, on convertit en tableau numpy\n",
        "    img_np = np.array(pil_img)\n",
        "\n",
        "    # Gestion de cas particuliers de format d'image :\n",
        "    # - Si l'image est en niveaux de gris (2D : H x W), on la convertit en RGB\n",
        "    if img_np.ndim == 2:  # (H, W)\n",
        "        img_np = cv2.cvtColor(img_np, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "    # - Si l'image est en RGBA, on la convertit en RGB\n",
        "    elif img_np.shape[2] == 4:  # (H, W, 4)\n",
        "        img_np = cv2.cvtColor(img_np, cv2.COLOR_RGBA2RGB)\n",
        "\n",
        "    # Conversion de l'espace de couleur RGB vers LAB.\n",
        "    # LAB sépare la luminosité (L) de la chrominance (a, b),\n",
        "    # ce qui permet d'appliquer le contraste uniquement sur la lumière.\n",
        "    lab = cv2.cvtColor(img_np, cv2.COLOR_RGB2LAB)\n",
        "\n",
        "    #https://docs.opencv.org/4.x/d8/d01/group__imgproc__color__conversions.html -> doc pour la gestion des conversion d'espaces de couleurs\n",
        "    #https://opencv.org/blog/color-spaces-in-opencv/ -> Pour comprendre ce qu'est LAB\n",
        "\n",
        "    # Sépare les trois canaux : L (luminosité), a et b (couleurs)\n",
        "    l, a, b = cv2.split(lab)\n",
        "    #https://docs.opencv.org/4.x/d2/de8/group__core__array.html#ga8f6d378f9f8eebb5cb55cd3ae295a8eb -> doc pour la séparation des canaux,\n",
        "\n",
        "    # Création de l'objet CLAHE (Contrast Limited Adaptive Histogram Equalization)\n",
        "    # - clipLimit contrôle la limitation du contraste (évite la saturation du bruit)\n",
        "    # - tileGridSize définit la taille des zones (tuiles) sur lesquelles l'histogramme est égalisé localement\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "    #https://docs.opencv.org/4.x/d6/dc7/group__imgproc__hist.html#gaa2e859ccfb8d9c67c97e00fef5a9e5b5 -> doc pour la création de l'objet qui va appliquer la transformation\n",
        "\n",
        "    # Application du CLAHE uniquement sur le canal de luminosité L\n",
        "    cl = clahe.apply(l)\n",
        "\n",
        "    # On reconstruit l'image LAB en remplaçant L par sa version améliorée (cl)\n",
        "    limg = cv2.merge((cl, a, b))\n",
        "\n",
        "    # On reconvertit l'image de LAB vers RGB pour un affichage classique\n",
        "    final = cv2.cvtColor(limg, cv2.COLOR_LAB2RGB)\n",
        "\n",
        "    # On transforme à nouveau le tableau NumPy en image PIL pour rester cohérent avec le reste du code\n",
        "    return Image.fromarray(final)\n",
        "\n",
        "\n",
        "\n",
        "def polar_transform_tensor(\n",
        "    tensor: torch.Tensor,\n",
        "    size: Tuple[int, int],\n",
        "    is_mask: bool = False\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Transforme un tensor [C,H,W] ou [1,H,W] ou [H,W] en coordonnées polaires\n",
        "    autour du centre de l'image, via warpPolar d'OpenCV.\n",
        "\n",
        "    Args:\n",
        "        tensor: image ou masque au format Tensor [C,H,W], [1,H,W] ou [H,W].\n",
        "        size: taille de sortie (H_out, W_out).\n",
        "        is_mask: True pour un masque (interpolation NEAREST),\n",
        "                 False pour une image (interpolation bilinéaire).\n",
        "\n",
        "    Returns:\n",
        "        Tensor transformé [C,H_out,W_out].\n",
        "    \"\"\"\n",
        "\n",
        "    # Si on reçoit un tensor 2D [H,W], on ajoute une dimension canal -> [1,H,W]\n",
        "    if tensor.ndim == 2:\n",
        "        tensor = tensor.unsqueeze(0)  # [1,H,W]\n",
        "\n",
        "    # Déballage des dimensions : C = canaux, H = hauteur, W = largeur\n",
        "    c, h, w = tensor.shape\n",
        "\n",
        "    # Conversion Tensor -> NumPy avec permutation des axes :\n",
        "    # on passe de [C,H,W] à [H,W,C], format attendu par OpenCV.\n",
        "    # .cpu() pour s'assurer que les données sont sur le CPU\n",
        "    img_np = tensor.permute(1, 2, 0).cpu().numpy()\n",
        "\n",
        "    # Définition du centre de l'image (en coordonnées (x, y) = (col, row))\n",
        "    center = (w / 2.0, h / 2.0)\n",
        "\n",
        "    # Rayon maximal de la transformation polaire :\n",
        "    # on prend le plus petit demi-côté pour rester à l'intérieur de l'image.\n",
        "    max_radius = min(center[0], center[1])\n",
        "\n",
        "    # Choix de l'interpolation :\n",
        "    # - NEAREST pour les masques\n",
        "    # - BILINEAIRE pour les images\n",
        "    interp_flag = cv2.INTER_NEAREST if is_mask else cv2.INTER_LINEAR\n",
        "\n",
        "    # Construction des flags pour warpPolar :\n",
        "    # - WARP_POLAR_LINEAR : coordonnées polaires linéaires (r, theta)\n",
        "    # - WARP_FILL_OUTLIERS : remplit les zones hors source avec une valeur constante\n",
        "    # - + le type d'interpolation choisi\n",
        "    flags = cv2.WARP_POLAR_LINEAR + cv2.WARP_FILL_OUTLIERS + interp_flag\n",
        "\n",
        "    # Transformation en coordonnées polaires :\n",
        "    # sortie initiale de taille (w, h) en (width, height)\n",
        "    polar = cv2.warpPolar(img_np, (w, h), center, max_radius, flags)\n",
        "\n",
        "    # Rotation de 90° dans le sens horaire pour remettre l'axe angulaire/radial\n",
        "    # dans une orientation plus intuitive pour le reste de la pipeline\n",
        "    polar = cv2.rotate(polar, cv2.ROTATE_90_CLOCKWISE)\n",
        "\n",
        "    # Redimensionnement final vers la taille demandée (W_out, H_out)\n",
        "    # cv2.resize prend (width, height) et pas (height,width)\n",
        "    polar = cv2.resize(polar, (size[1], size[0]), interpolation=interp_flag)\n",
        "\n",
        "    # Si la sortie est devenue 2D (H,W) -> on rajoute un canal (H,W,1)\n",
        "    # pour garder la cohérence (H,W,C)\n",
        "    if polar.ndim == 2:\n",
        "        polar = np.expand_dims(polar, axis=-1)\n",
        "\n",
        "    # Retour au format PyTorch : (H,W,C) -> (C,H,W)\n",
        "    polar_tensor = torch.from_numpy(polar).permute(2, 0, 1)\n",
        "\n",
        "    return polar_tensor\n",
        "\n",
        "\n",
        "def compute_mean_std(\n",
        "    dataset: Dataset,\n",
        "    image_key: str = \"image\",\n",
        "    image_size: Optional[Tuple[int, int]] = None,\n",
        "    batch_size: int = 8,\n",
        "    num_workers: int = 2\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Calcule mean et std (par canal) sur un dataset de segmentation.\n",
        "\n",
        "    Le dataset doit retourner un dict contenant au moins:\n",
        "        sample[image_key] = Tensor [C,H,W] dans [0,1].\n",
        "\n",
        "    Args:\n",
        "        dataset: dataset PyTorch.\n",
        "        image_key: nom de la clef image dans le sample.\n",
        "        image_size: si non None, resize avant calcul (H,W).\n",
        "        batch_size: taille du batch pour le DataLoader.\n",
        "        num_workers: workers pour le DataLoader.\n",
        "\n",
        "    Returns:\n",
        "        (mean, std): deux tenseurs 1D de taille C.\n",
        "    \"\"\"\n",
        "\n",
        "    # Création d'un DataLoader pour itérer sur le dataset par batch,\n",
        "    loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    # Variables qui seront initialisées au premier batch\n",
        "    n_channels = None\n",
        "    channel_sum = None\n",
        "    channel_sq_sum = None\n",
        "    num_images = 0\n",
        "\n",
        "\n",
        "    # Parcours de tous les batches du DataLoader\n",
        "    for batch in tqdm(loader, desc=\"Stats\"):\n",
        "        # Récupère le tenseur image dans le batch: [B,C,H,W]\n",
        "        images = batch[image_key]\n",
        "\n",
        "\n",
        "        # TF.resize accepte les batches de forme [B,C,H,W]\n",
        "        if image_size is not None:\n",
        "            images = TF.resize(images, image_size, InterpolationMode.BILINEAR\n",
        ")\n",
        "\n",
        "        # B = taille du batch, C = canaux, H/W = dimensions spatiales\n",
        "        b, c, h, w = images.shape\n",
        "\n",
        "        # Initialisation des accumulateurs au premier passage (pour connaître C)\n",
        "        if n_channels is None:\n",
        "            n_channels = c\n",
        "            channel_sum = torch.zeros(c)\n",
        "            channel_sq_sum = torch.zeros(c)\n",
        "\n",
        "        # Aplatissement spatial: [B,C,H,W] -> [B,C,H*W]\n",
        "        # On regroupe tous les pixels de chaque image dans une seule dimension.\n",
        "        images = images.view(b, c, -1)\n",
        "\n",
        "        # mean(dim=2) -> moyenne par canal pour chaque image: [B,C]\n",
        "        # sum(dim=0)  -> somme des moyennes sur toutes les images du batch: [C]\n",
        "        channel_sum += images.mean(dim=2).sum(dim=0)\n",
        "\n",
        "        # Même chose mais pour les valeurs au carré: on accumule la moyenne\n",
        "        # des carrés des pixels, pour pouvoir calculer la variance ensuite.\n",
        "        channel_sq_sum += (images ** 2).mean(dim=2).sum(dim=0)\n",
        "\n",
        "        # Incrémente le nombre d'images vues\n",
        "        num_images += b\n",
        "\n",
        "    # Moyenne finale par canal:\n",
        "    # on divise la somme des moyennes par le nombre d'images\n",
        "    mean = channel_sum / num_images\n",
        "\n",
        "    # Variance = E[X^2] - (E[X])^2, puis racine pour obtenir (std)\n",
        "    std = (channel_sq_sum / num_images - mean ** 2).sqrt()\n",
        "\n",
        "    print(f\"  -> mean: {mean}\")\n",
        "    print(f\"  -> std : {std}\")\n",
        "    return mean, std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30YwA5HowNuk"
      },
      "outputs": [],
      "source": [
        "\n",
        "class RetinalSegmentationTransform:\n",
        "    \"\"\"\n",
        "    Pipeline de transformations généraliser pour tous les datasets qu'on utilisera.\n",
        "\n",
        "    - Resize image + masques à image_size\n",
        "    - Si train=True : flips / rotation aléatoires\n",
        "    - Si use_polar=True : transformation polaire\n",
        "    - Normalisation de l'image avec (mean, std) calculé avec compute_mean_std() du dessus\n",
        "\n",
        "     Args:\n",
        "        image  : Tensor [3,H,W] en [0,1]\n",
        "        masks  : dict[str, Tensor] (ex: {\"fov\": [1,H,W], \"gt\": [1,H,W]})\n",
        "\n",
        "    Returns:\n",
        "        image_tensor, masks_dict\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_size: Tuple[int, int],\n",
        "        mean: torch.Tensor,\n",
        "        std: torch.Tensor,\n",
        "        train: bool = True,\n",
        "        use_polar: bool = False,\n",
        "        rotation_deg: float = 15.0\n",
        "    ):\n",
        "\n",
        "        self.image_size = image_size\n",
        "\n",
        "\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "\n",
        "        self.train = train\n",
        "\n",
        "\n",
        "        self.use_polar = use_polar\n",
        "\n",
        "\n",
        "        self.rotation_deg = rotation_deg\n",
        "\n",
        "    def _resize(self, tensor: torch.Tensor, is_mask: bool) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Redimensionne image ou masque à self.image_size.\n",
        "        \"\"\"\n",
        "\n",
        "        # Pour les masques: NEAREST (évite d'interpoler les labels)\n",
        "        # Pour les images: BILINEAR (résultat plus lisse)\n",
        "        interpolation = (\n",
        "            InterpolationMode.NEAREST if is_mask else InterpolationMode.BILINEAR\n",
        "\n",
        "        )\n",
        "\n",
        "        return TF.resize(tensor, self.image_size, interpolation=interpolation)\n",
        "\n",
        "    def _maybe_augment(\n",
        "        self,\n",
        "        image: torch.Tensor,\n",
        "        masks: Dict[str, torch.Tensor]\n",
        "    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
        "        \"\"\"Applique flips / rotation aléatoires sur les images et masks.\"\"\"\n",
        "\n",
        "        # Si on n'est pas en mode entraînement, aucune augmentation\n",
        "        if not self.train:\n",
        "            return image, masks\n",
        "\n",
        "        # Flip horizontal aléatoire\n",
        "        if torch.rand(1).item() < 0.5:\n",
        "            image = TF.hflip(image)\n",
        "            # On applique EXACTEMENT la même transformation à tous les masques\n",
        "            masks = {k: TF.hflip(v) for k, v in masks.items()}\n",
        "\n",
        "        # Flip vertical aléatoire\n",
        "        if torch.rand(1).item() < 0.5:\n",
        "            image = TF.vflip(image)\n",
        "            masks = {k: TF.vflip(v) for k, v in masks.items()}\n",
        "\n",
        "        # Rotation aléatoire dans [-rotation_deg, +rotation_deg] 15 par défaut\n",
        "        angle = float(torch.empty(1).uniform_(-self.rotation_deg, self.rotation_deg))\n",
        "\n",
        "\n",
        "        # Pour l'image : interpolation bilinéaire (BILINEAR) → plus lisse\n",
        "        image = TF.rotate(image, angle, interpolation=InterpolationMode.BILINEAR\n",
        ")\n",
        "\n",
        "\n",
        "        masks = {\n",
        "            k: TF.rotate(v, angle, interpolation=InterpolationMode.NEAREST)\n",
        "            for k, v in masks.items()\n",
        "        }\n",
        "\n",
        "        return image, masks\n",
        "\n",
        "    def _maybe_polar(\n",
        "        self,\n",
        "        image: torch.Tensor,\n",
        "        masks: Dict[str, torch.Tensor]\n",
        "    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
        "        \"\"\"\n",
        "        Applique la transformation polaire (cartésien -> polaire) sur image et masques\n",
        "        si use_polar=True. Utile pour ORIGA ou le papier dit que pour améliorer les résultats, cette transformation est nécessaire.\n",
        "        \"\"\"\n",
        "        if not self.use_polar:\n",
        "            return image, masks\n",
        "\n",
        "        # Image en polaire (interpolation d'image)\n",
        "        image = polar_transform_tensor(\n",
        "            image, size=self.image_size, is_mask=False\n",
        "        )\n",
        "\n",
        "        # Tous les masques en polaire, avec interpolation de type masque\n",
        "        masks = {\n",
        "            k: polar_transform_tensor(v, size=self.image_size, is_mask=True)\n",
        "            for k, v in masks.items()\n",
        "        }\n",
        "        return image, masks\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        image: torch.Tensor,\n",
        "        masks: Dict[str, torch.Tensor]\n",
        "    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n",
        "        \"\"\"\n",
        "        Applique  :\n",
        "        1) Resize\n",
        "        2) Augmentations géométriques (si train)\n",
        "        3) Transformation polaire (si use_polar)\n",
        "        4) Normalisation (mean/std)\n",
        "        \"\"\"\n",
        "\n",
        "        # Resize commun et obligatoire sur image et masques\n",
        "        image = self._resize(image, is_mask=False)\n",
        "        masks = {k: self._resize(v, is_mask=True) for k, v in masks.items()}\n",
        "\n",
        "        # Augmentations géométriques image/masques\n",
        "        image, masks = self._maybe_augment(image, masks)\n",
        "\n",
        "        #  passage en coordonnées polaires pour ORIGA et REFUGE\n",
        "        image, masks = self._maybe_polar(image, masks)\n",
        "\n",
        "        #  Normalisation de l'image\n",
        "        image = TF.normalize(image, self.mean, self.std)\n",
        "\n",
        "        return image, masks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Yk6csFa77Rb"
      },
      "outputs": [],
      "source": [
        "def show_drive_val_sample(\n",
        "    model,\n",
        "    val_loader,\n",
        "    device,\n",
        "    title_prefix=\"DRIVE - epoch\",\n",
        "    threshold: float | None = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Affiche, pour 1 image du set de validation :\n",
        "      - structure S\n",
        "      - texture |T|\n",
        "      - extracted structure (canal vert uniquement)\n",
        "      - segmentation binaire avec vaisseaux en rouge\n",
        "\n",
        "    On prend simplement le premier batch du loader.\n",
        "    \"\"\"\n",
        "\n",
        "    # Passage du modèle en mode évaluation :\n",
        "    # - désactive les comportements spécifiques au training\n",
        "    model.eval()\n",
        "\n",
        "    # Récupération du premier batch du val_loader\n",
        "    batch = next(iter(val_loader))\n",
        "    img  = batch[\"image\"].to(device)   #  image RGB normalisée\n",
        "    gt   = batch[\"gt\"].to(device)      #  masque de vérité terrain\n",
        "    fov  = batch.get(\"fov\", None)      #  masque de FOV\n",
        "\n",
        "    # Si un masque FOV est présent, on le déplace aussi sur le bon device (CPU/GPU)\n",
        "    if fov is not None:\n",
        "        fov = fov.to(device)\n",
        "\n",
        "    # On désactive le calcul du gradient\n",
        "    with torch.no_grad():\n",
        "        #On récupère les sorties du modèles qu'on veux afficher (segmentation,texture,extracted_structure et la structure)\n",
        "        seg, _, _, _, texture, extracted_structure, structure = model(img)\n",
        "\n",
        "        # On applique un sigmoid pour convertir les logits en probabilités [0,1]\n",
        "        prob = torch.sigmoid(seg)      # [B,1,H,W]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # On ne visualise que le premier élément du batch\n",
        "    S  = structure[0].cpu()\n",
        "    T  = texture[0].cpu()\n",
        "    E  = extracted_structure[0].cpu()\n",
        "    P  = prob[0, 0].cpu()\n",
        "\n",
        "    H, W = P.shape\n",
        "\n",
        "\n",
        "    if fov is not None:\n",
        "        F = fov[0, 0].cpu()\n",
        "    else:\n",
        "        F = torch.ones_like(P)\n",
        "\n",
        "    # ---------- 1) Structure S (normalisée) ----------\n",
        "    S_vis = S.clone()\n",
        "    # Normalisation min-max pour ramener S_vis dans [0,1] car S est normalisé pour le réseau et peux contenur des valeurs négfatives. Pour faciliter l'affichage on remet entre 0 et 1\n",
        "    # Article -> https://fr.statisticseasily.com/glossaire/Qu%27est-ce-que-la-normalisation-min-max-expliqu%C3%A9e/\n",
        "    s_min, s_max = S_vis.min(), S_vis.max()\n",
        "    S_vis = (S_vis - s_min) / (s_max - s_min + 1e-8)\n",
        "\n",
        "    # Application du masque FOV : on met à zéro les pixels hors FOV\n",
        "    S_vis = S_vis * F.unsqueeze(0)     # [3,H,W]\n",
        "\n",
        "    # Permutation en [H,W,3] pour être compatible avec plt.imshow\n",
        "    S_vis = S_vis.permute(1, 2, 0)     # [H,W,3]\n",
        "\n",
        "    # ---------- 2) Texture |T| (moyenne des canaux) ----------\n",
        "    # On prend la valeur absolue de T puis on moyenne les 3 canaux → carte scalaire\n",
        "    T_abs = T.abs().mean(0)            # [H,W]\n",
        "    # Normalisation min-max\n",
        "    t_min, t_max = T_abs.min(), T_abs.max()\n",
        "    T_vis = (T_abs - t_min) / (t_max - t_min + 1e-8)\n",
        "    # Masquage FOV\n",
        "    T_vis = T_vis * F                  # [H,W]\n",
        "\n",
        "    # ---------- 3) Extracted structure – canal vert uniquement ----------\n",
        "    # On ne visualise que le canal vert (indice 1) de la structure extraite\n",
        "    E_green = E[1]                     # [H,W]\n",
        "    e_min, e_max = E_green.min(), E_green.max()\n",
        "    E_vis = (E_green - e_min) / (e_max - e_min + 1e-8)\n",
        "    E_vis = E_vis * F                  # [H,W]\n",
        "\n",
        "    # ---------- 4) Segmentation binaire ----------\n",
        "    if threshold is None:\n",
        "        # Si aucun seuil n'est fourni, on affiche directement les probabilités :\n",
        "        # plus la proba est élevée, plus le rouge est intense\n",
        "        mask = P * F                   # [H,W]\n",
        "    else:\n",
        "        # Sinon, on binarise à partir du seuil donné (0/1)\n",
        "        mask = (P >= threshold).float() * F\n",
        "\n",
        "    seg = mask\n",
        "\n",
        "\n",
        "    # ---------- AFFICHAGE ----------\n",
        "    # On crée une figure avec 1 ligne et 4 colonnes de sous-figures\n",
        "    plt.figure(figsize=(16, 4))\n",
        "\n",
        "    # 1) Structure S\n",
        "    plt.subplot(1, 4, 1)\n",
        "    plt.imshow(S_vis.numpy())\n",
        "    plt.title(f\"{title_prefix} - Structure\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    # 2) Texture |T|\n",
        "    plt.subplot(1, 4, 2)\n",
        "    plt.imshow(T_vis.numpy(), cmap=\"gray\")\n",
        "    plt.title(\"Texture\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    # 3) Structure extraite, canal vert\n",
        "    plt.subplot(1, 4, 3)\n",
        "    plt.imshow(E_vis.numpy(), cmap=\"gray\")\n",
        "    plt.title(\"Extracted structure (Canal vert)\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    # 4) Segmentation (vaisseaux en rouge)\n",
        "    plt.subplot(1, 4, 4)\n",
        "    plt.imshow(seg.numpy(), cmap=\"gray\", vmin=0, vmax=1)\n",
        "    plt.title(\"Segmentation\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "\n",
        "    # Ajuste les marges entre les sous-figures\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9Xt_37QwNul"
      },
      "outputs": [],
      "source": [
        "class DRIVEDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset DRIVE.\n",
        "\n",
        "    - images_dir : chemin vers images RGB (.tif)\n",
        "    - fov_dir    : masque FOV (pixels valides)\n",
        "    - gt_dir     : masques 1er annotateur\n",
        "    - second_gt_dir : masques 2e annotateur\n",
        "    - transform  : instance de RetinalSegmentationTransform ou None\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        images_dir: str,\n",
        "        fov_dir: str,\n",
        "        gt_dir: Optional[str] = None,\n",
        "        second_gt_dir: Optional[str] = None,\n",
        "        split: str = \"train\",\n",
        "        transform: Optional[RetinalSegmentationTransform] = None,\n",
        "        apply_clahe_flag: bool = True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # On stocke les chemins principaux (images / FOV / GT)\n",
        "        self.images_dir = Path(images_dir)\n",
        "        self.fov_dir = Path(fov_dir)\n",
        "        self.gt_dir = Path(gt_dir) if gt_dir is not None else None\n",
        "        self.second_gt_dir = Path(second_gt_dir) if second_gt_dir is not None else None\n",
        "\n",
        "        # 'train' ou 'test' pour DRIVE\n",
        "        self.split = split\n",
        "\n",
        "        # Transform commun\n",
        "        self.transform = transform\n",
        "\n",
        "        # Active/désactive le prétraitement CLAHE sur les images RGB\n",
        "        self.apply_clahe_flag = apply_clahe_flag\n",
        "\n",
        "        # On liste tous les fichiers .tif d'images\n",
        "        self.image_filenames = sorted(\n",
        "            [f for f in os.listdir(self.images_dir) if f.endswith(\".tif\")]\n",
        "        )\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        # Nombre d'images = taille du dataset\n",
        "        return len(self.image_filenames)\n",
        "\n",
        "    def _get_mask_names(self, img_name: str) -> Tuple[str, str, Optional[str]]:\n",
        "        \"\"\"\n",
        "        DRIVE encode TRAIN / TEST dans le nom des fichiers.\n",
        "        On reconstruit les noms des masques FOV + GT à partir du nom d'image.\n",
        "        \"\"\"\n",
        "        base = img_name.split(\"_\")[0]\n",
        "\n",
        "        # FOV : suffix différent selon train/test\n",
        "        if self.split == \"train\":\n",
        "            fov_name = f\"{base}_training_mask.gif\"\n",
        "        else:\n",
        "            fov_name = f\"{base}_test_mask.gif\"\n",
        "\n",
        "\n",
        "        gt1_name = f\"{base}_manual1.gif\"\n",
        "        gt2_name = f\"{base}_manual2.gif\" if self.second_gt_dir is not None else None\n",
        "        return fov_name, gt1_name, gt2_name\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
        "        # Nom de l'image indexée\n",
        "        img_name = self.image_filenames[idx]\n",
        "        fov_name, gt1_name, gt2_name = self._get_mask_names(img_name)\n",
        "\n",
        "        # --- Image RGB ---\n",
        "        img = Image.open(self.images_dir / img_name).convert(\"RGB\")\n",
        "\n",
        "        if self.apply_clahe_flag:\n",
        "          #Application de CLAHE\n",
        "            img = apply_clahe(img)\n",
        "\n",
        "        # Conversion en tensor float [3,H,W] dans [0,1]\n",
        "        img_tensor = TF.to_tensor(img)\n",
        "\n",
        "        # --- FOV ---\n",
        "        fov = Image.open(self.fov_dir / fov_name)\n",
        "        fov_tensor = TF.to_tensor(fov)\n",
        "\n",
        "        # --- GT1 (vaisseaux) ---\n",
        "        gt1_tensor = None\n",
        "        if self.gt_dir is not None:\n",
        "            gt1 = Image.open(self.gt_dir / gt1_name)\n",
        "            gt1_tensor = TF.to_tensor(gt1)\n",
        "\n",
        "        # --- GT2  ---\n",
        "        gt2_tensor = None\n",
        "        if self.second_gt_dir is not None and gt2_name is not None:\n",
        "            gt2_path = self.second_gt_dir / gt2_name\n",
        "            #certains fichier manquant, vérification s'ils existe\n",
        "            if gt2_path.exists():\n",
        "                gt2 = Image.open(gt2_path)\n",
        "                gt2_tensor = TF.to_tensor(gt2)\n",
        "\n",
        "        # Rassemblement de tout dans un dict\n",
        "        masks = {\"fov\": fov_tensor}\n",
        "        if gt1_tensor is not None:\n",
        "            masks[\"gt\"] = gt1_tensor\n",
        "        if gt2_tensor is not None:\n",
        "            masks[\"gt2\"] = gt2_tensor\n",
        "\n",
        "        # --- Transformations  ---\n",
        "        if self.transform is not None:\n",
        "            img_tensor, masks = self.transform(img_tensor, masks)\n",
        "\n",
        "        # Binarisation des masques (seuil 0.5) -> Après toTensor, on a des valeurs continu entre 0 et 1 sauf qu'on veut nous dans notre mask soit 0 soit 1 on fait alors la binarisation\n",
        "        fov_bin = (masks[\"fov\"] > 0.5).float()\n",
        "\n",
        "        sample = {\n",
        "            \"image\": img_tensor,\n",
        "            \"fov\": fov_bin,\n",
        "            \"filename\": img_name,\n",
        "        }\n",
        "\n",
        "        if \"gt\" in masks:\n",
        "            sample[\"gt\"] = (masks[\"gt\"] > 0.5).float()\n",
        "        if \"gt2\" in masks:\n",
        "            sample[\"gt2\"] = (masks[\"gt2\"] > 0.5).float()\n",
        "\n",
        "        return sample\n",
        "\n",
        "\n",
        "\n",
        "class ORIGADataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset ORIGA pour segmentation du disque et de la cup,\n",
        "\n",
        "    On lit un masque de labels (0: background, 1: disc, 2: cup),\n",
        "    puis on produit deux canaux binaires:\n",
        "        - canal 0 = disque\n",
        "        - canal 1 = cup\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        csv_path: str,\n",
        "        images_dir: str,\n",
        "        masks_dir: str,\n",
        "        split: str = \"train\",\n",
        "        transform: Optional[RetinalSegmentationTransform] = None,\n",
        "        disc_label_val: int = 1,\n",
        "        cup_label_val: int = 2,\n",
        "        image_extensions: Tuple[str, ...] = (\"jpg\", \"png\", \"bmp\", \"tif\"),\n",
        "        mask_extensions: Tuple[str, ...] = (\"png\", \"bmp\", \"jpg\", \"tif\"),\n",
        "        seed: int = 42,\n",
        "        output_image_size: Optional[Tuple[int, int]] = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        self.images_dir = Path(images_dir)\n",
        "        self.masks_dir = Path(masks_dir)\n",
        "\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "\n",
        "        self.disc_label_val = float(disc_label_val)\n",
        "        self.cup_label_val = float(cup_label_val)\n",
        "\n",
        "        self.image_extensions = image_extensions\n",
        "        self.mask_extensions = mask_extensions\n",
        "\n",
        "\n",
        "        self.output_image_size = output_image_size\n",
        "\n",
        "        # CSV + split ---\n",
        "        df = pd.read_csv(csv_path)\n",
        "        train_df, test_df = self._build_official_split(df, seed=seed)\n",
        "\n",
        "        # Choix du split (train/test)\n",
        "        if split == \"train\":\n",
        "            self.df = train_df.reset_index(drop=True)\n",
        "        elif split == \"test\":\n",
        "            self.df = test_df.reset_index(drop=True)\n",
        "        else:\n",
        "            raise ValueError(\"split doit être 'train' ou 'test'\")\n",
        "\n",
        "\n",
        "        self.filenames: List[str] = self.df[\"Filename\"].tolist()\n",
        "        self.glaucoma_labels: List[int] = self.df[\"Glaucoma\"].astype(int).tolist()\n",
        "        self.exp_cdr: np.ndarray = self.df[\"ExpCDR\"].values\n",
        "\n",
        "    # ---- Gestion du split  ORIGA ----\n",
        "    @staticmethod\n",
        "    def _build_official_split(df: pd.DataFrame, seed: int = 42) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Reproduit le split utilisé dans le papier si les infos sont dans la colonne 'Set'.\n",
        "        Sinon crée un split cohérent (73 glaucomateux / 252 non glaucomateux).\n",
        "        \"\"\"\n",
        "        uniques = df.get(\"Set\", pd.Series([])).unique()\n",
        "        train_df, test_df = None, None\n",
        "\n",
        "        # On tente de retrouver les deux sous-ensembles\n",
        "        for u in uniques:\n",
        "            cand = df[df[\"Set\"] == u]\n",
        "            if len(cand) == 325 and cand[\"Glaucoma\"].sum() == 73:\n",
        "                train_df = cand\n",
        "            if len(cand) == 325 and cand[\"Glaucoma\"].sum() == 95:\n",
        "                test_df = cand\n",
        "\n",
        "        # Si on les a trouvés, on les renvoie\n",
        "        if train_df is not None and test_df is not None:\n",
        "            return train_df, test_df\n",
        "\n",
        "        # Sinon: fallback = split stratifié reproductible\n",
        "        rng = np.random.RandomState(seed)\n",
        "        pos = df[df[\"Glaucoma\"] == 1].copy()\n",
        "        neg = df[df[\"Glaucoma\"] == 0].copy()\n",
        "\n",
        "        # On mélange séparément positifs et négatifs\n",
        "        pos = pos.sample(frac=1.0, random_state=seed)\n",
        "        neg = neg.sample(frac=1.0, random_state=seed + 1)\n",
        "\n",
        "        # On reconstitue le train / test avec les nombres classiques\n",
        "        train_pos = pos.iloc[:73]\n",
        "        test_pos = pos.iloc[73:]\n",
        "        train_neg = neg.iloc[:252]\n",
        "        test_neg = neg.iloc[252:]\n",
        "\n",
        "        train_df = pd.concat([train_pos, train_neg]).sample(frac=1.0, random_state=seed).reset_index(drop=True)\n",
        "        test_df = pd.concat([test_pos, test_neg]).sample(frac=1.0, random_state=seed + 2).reset_index(drop=True)\n",
        "        return train_df, test_df\n",
        "\n",
        "    def _find_with_extensions(\n",
        "        self,\n",
        "        directory: Path,\n",
        "        filename: str,\n",
        "        extensions: Tuple[str, ...]\n",
        "    ) -> Path:\n",
        "        \"\"\"\n",
        "        ORIGA stocke les fichiers avec diverses extensions.\n",
        "        Cette fonction cherche la bonne extension automatiquement.\n",
        "        \"\"\"\n",
        "        # Chemin tel quel\n",
        "        p = directory / filename\n",
        "        if p.exists():\n",
        "            return p\n",
        "\n",
        "        # Sinon, on tente toutes les extensions possibles\n",
        "        stem, _ = os.path.splitext(filename)\n",
        "        for ext in extensions:\n",
        "            cand = directory / f\"{stem}.{ext}\"\n",
        "            if cand.exists():\n",
        "                return cand\n",
        "\n",
        "        raise FileNotFoundError(f\"{filename} not found in {directory}\")\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.filenames)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
        "        row = self.df.iloc[idx]\n",
        "        fname = row[\"Filename\"]\n",
        "\n",
        "        # --- Image ---\n",
        "        img_path = self._find_with_extensions(\n",
        "            self.images_dir, fname, self.image_extensions\n",
        "        )\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        # img = apply_clahe(img)\n",
        "\n",
        "        # --- Masque label (0,1,2) ---\n",
        "        mask_path = self._find_with_extensions(\n",
        "            self.masks_dir, fname, self.mask_extensions\n",
        "        )\n",
        "        mask = Image.open(mask_path).convert(\"L\")  # niveaux de gris: 0/1/2\n",
        "\n",
        "\n",
        "        if self.output_image_size is not None and self.transform is None:\n",
        "            img = img.resize(self.output_image_size, Image.BILINEAR)\n",
        "            mask = mask.resize(self.output_image_size, Image.NEAREST)\n",
        "\n",
        "        # PIL -> Tensor [3,H,W] en [0,1]\n",
        "        img_tensor = TF.to_tensor(img)\n",
        "\n",
        "        # Masque en numpy pour garder les valeurs 0,1,2\n",
        "        mask_np = np.array(mask, dtype=np.float32)\n",
        "        mask_tensor = torch.from_numpy(mask_np).unsqueeze(0)\n",
        "\n",
        "        masks = {\"mask\": mask_tensor}\n",
        "\n",
        "        # --- Transformations communes  ---\n",
        "        if self.transform is not None:\n",
        "            img_tensor, masks = self.transform(img_tensor, masks)\n",
        "\n",
        "        # On récupère le masque transformé\n",
        "        mask_tensor = masks[\"mask\"].squeeze(0)  # [H,W]\n",
        "\n",
        "        # --- Construction des masques binaires disque / cup ---\n",
        "        disc = (mask_tensor == self.disc_label_val).float().unsqueeze(0)\n",
        "        cup = (mask_tensor == self.cup_label_val).float().unsqueeze(0)\n",
        "        gt = torch.cat([disc, cup], dim=0)\n",
        "\n",
        "        # FOV : ici, on considère tout le champ comme valide\n",
        "        fov = torch.ones_like(disc)\n",
        "\n",
        "        return {\n",
        "            \"image\": img_tensor,\n",
        "            \"gt\": gt,\n",
        "            \"disc\": disc,\n",
        "            \"cup\": cup,\n",
        "            \"fov\": fov,\n",
        "            \"filename\": fname,\n",
        "            \"glaucoma\": int(row[\"Glaucoma\"]),\n",
        "            \"cdr\": float(row[\"ExpCDR\"]),\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y810WOHB2eQI"
      },
      "outputs": [],
      "source": [
        "class STDModule(nn.Module):\n",
        "    \"\"\"\n",
        "    Module Structure-Texture Demixing (STD).\n",
        "    10 convolutions 3x3 + LeakyReLU pour extraire une composante de texture T.\n",
        "\n",
        "    On pose: S = I - T (structure).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, img_channels: int = 3, hidden_dim: int = 64):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = []\n",
        "\n",
        "        # Première couche : image -> features\n",
        "        layers.append(nn.Conv2d(img_channels, hidden_dim, kernel_size=3, padding=1))\n",
        "        layers.append(nn.LeakyReLU(inplace=True))\n",
        "\n",
        "        # 8 couches internes\n",
        "        for _ in range(8):\n",
        "            layers.append(nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, padding=1))\n",
        "            layers.append(nn.LeakyReLU(inplace=True))\n",
        "\n",
        "        # Dernière couche : features -> texture T\n",
        "        layers.append(nn.Conv2d(hidden_dim, img_channels, kernel_size=3, padding=1))\n",
        "\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, input_image: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        texture = self.net(input_image)\n",
        "        structure = input_image - texture\n",
        "        return structure, texture\n",
        "\n",
        "\n",
        "class AdaptiveNorm(nn.Module):\n",
        "    \"\"\"\n",
        "    Adaptive Normalization :\n",
        "        Psi(x) = lambda * x + mu * BN(x)\n",
        "\n",
        "    - lambda et mu sont des scalaires appris\n",
        "    - BN est une BatchNorm2d standard\n",
        "    Source : https://arxiv.org/pdf/1709.00643\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_channels: int):\n",
        "        super().__init__()\n",
        "        self.bn = nn.BatchNorm2d(num_channels, affine=True)\n",
        "\n",
        "        # On commence avec une identité (lambda=1, mu=0)\n",
        "        self.lambda_s = nn.Parameter(torch.tensor(1.0))\n",
        "        self.mu_s = nn.Parameter(torch.tensor(0.0))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.lambda_s * x + self.mu_s * self.bn(x)\n",
        "\n",
        "\n",
        "class TextureBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Bloc 1x1 du papier:\n",
        "        Conv(1x1) -> AdaptiveNorm -> LeakyReLU -> Conv(1x1)\n",
        "\n",
        "    Sert à extraire une composante de structure additionnelle à partir de la texture.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, img_channels: int = 3, hidden_channels: int = 15):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(img_channels, hidden_channels, kernel_size=1)\n",
        "        self.adapt = AdaptiveNorm(hidden_channels)\n",
        "        self.act = nn.LeakyReLU(inplace=False)\n",
        "        self.conv2 = nn.Conv2d(hidden_channels, img_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, texture: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.conv1(texture)\n",
        "        x = self.adapt(x)\n",
        "        x = self.act(x)\n",
        "        extracted_structure = self.conv2(x)\n",
        "        return extracted_structure\n",
        "\n",
        "\n",
        "def double_conv(in_channels: int, out_channels: int) -> nn.Sequential:\n",
        "    \"\"\"\n",
        "    Bloc de base de type U-Net / M-Net:\n",
        "        Conv3x3 -> ReLU -> Conv3x3 -> ReLU\n",
        "    \"\"\"\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "class MNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder-décodeur multi-niveaux (M-Net) avec deep supervision\n",
        "    et fusion de la texture.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size: int,\n",
        "        img_channels: int,\n",
        "        texture_channels: int = 3,\n",
        "        num_classes: int = 1, # Added num_classes parameter\n",
        "\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # -------------------- ENCODEUR --------------------\n",
        "        self.enc1 = double_conv(img_channels, 32)\n",
        "        self.down1 = nn.MaxPool2d(2)\n",
        "\n",
        "        self.resize2 = double_conv(img_channels, 64)\n",
        "        self.enc2 = double_conv(96, 64)\n",
        "        self.down2 = nn.MaxPool2d(2)\n",
        "\n",
        "        self.resize3 = double_conv(img_channels, 128)\n",
        "        self.enc3 = double_conv(192, 128)\n",
        "        self.down3 = nn.MaxPool2d(2)\n",
        "\n",
        "        self.resize4 = double_conv(img_channels, 256)\n",
        "        self.enc4 = double_conv(384, 256)\n",
        "        self.down4 = nn.MaxPool2d(2)\n",
        "\n",
        "        # -------------------- BOTTLE NECK --------------------\n",
        "        self.bottleneck = double_conv(256, 512)\n",
        "\n",
        "        # -------------------- DECODEUR --------------------\n",
        "        self.up5 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
        "        self.dec5 = double_conv(512, 256)\n",
        "        self.out5 = nn.ConvTranspose2d(256, num_classes, kernel_size=2, stride=2) # Used num_classes\n",
        "\n",
        "        self.up6 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.dec6 = double_conv(256, 128)\n",
        "        self.out6 = nn.ConvTranspose2d(128, num_classes, kernel_size=2, stride=2) # Used num_classes\n",
        "\n",
        "        self.up7 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.dec7 = double_conv(128, 64)\n",
        "        self.out7 = nn.ConvTranspose2d(64, num_classes, kernel_size=2, stride=2) # Used num_classes\n",
        "\n",
        "        self.up8 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
        "        self.dec8 = double_conv(64, 32)\n",
        "\n",
        "        # Fusion texture + features chemin principal\n",
        "        self.fuse_texture = nn.Conv2d(32 + texture_channels, num_classes, kernel_size=3, padding=1) # Used num_classes\n",
        "\n",
        "        # Fusion des sorties (deep supervision)\n",
        "        self.final_fusion = nn.Conv2d(4 * num_classes, num_classes, kernel_size=1) # Adjusted input channels and used num_classes\n",
        "\n",
        "        #Voir le schéma dans le papier pour bien comprendre\n",
        "\n",
        "    def _resize_structure(self, structure: torch.Tensor, scale: float) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Redimensionne la carte de structure par un facteur donné (scale) en H et W,\n",
        "        en utilisant une interpolation bilinéaire.\n",
        "\n",
        "        Args:\n",
        "            structure: tenseur [B,C,H,W] (typique pour des features d'un réseau).\n",
        "            scale: facteur d'échelle spatial\n",
        "\n",
        "        Returns:\n",
        "            Tenseur redimensionné [B,C,H',W'] avec H' = scale*H, W' = scale*W.\n",
        "        \"\"\"\n",
        "        return F.interpolate(\n",
        "            structure,\n",
        "            scale_factor=scale,     # multiplie la hauteur et la largeur par 'scale'\n",
        "            mode=\"bilinear\",        # interpolation bilinear\n",
        "            align_corners=False,    # convention\n",
        "            recompute_scale_factor=True,\n",
        "        )\n",
        "\n",
        "\n",
        "    def _upsample_and_concat(\n",
        "        self,\n",
        "        skip: torch.Tensor,\n",
        "        x: torch.Tensor,\n",
        "        up_module: nn.Module\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Upsample x puis concatène avec skip connection.\n",
        "        \"\"\"\n",
        "        up = up_module(x)\n",
        "        return torch.cat([skip, up], dim=1)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        structure: torch.Tensor,\n",
        "        extracted_texture: torch.Tensor\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        # -------------------- ENCODEUR --------------------\n",
        "        enc1 = self.enc1(structure)\n",
        "        down1 = self.down1(enc1)\n",
        "\n",
        "        #Pour le niveau 2 on divise la taille par 2 de la structure\n",
        "        structure_lvl2 = self._resize_structure(structure, 0.5)\n",
        "        prep2 = self.resize2(structure_lvl2)\n",
        "        enc2 = self.enc2(torch.cat([prep2, down1], dim=1))\n",
        "        down2 = self.down2(enc2)\n",
        "        #Pour le niveau 3 on divise par 2 l'entrée précédente\n",
        "        structure_lvl3 = self._resize_structure(structure_lvl2, 0.5)\n",
        "        prep3 = self.resize3(structure_lvl3)\n",
        "        enc3 = self.enc3(torch.cat([prep3, down2], dim=1))\n",
        "        down3 = self.down3(enc3)\n",
        "        #Pareil ici\n",
        "        structure_lvl4 = self._resize_structure(structure_lvl3, 0.5)\n",
        "        prep4 = self.resize4(structure_lvl4)\n",
        "        enc4 = self.enc4(torch.cat([prep4, down3], dim=1))\n",
        "        down4 = self.down4(enc4)\n",
        "\n",
        "        # -------------------- BOTTLE NECK --------------------\n",
        "        bottleneck = self.bottleneck(down4)\n",
        "\n",
        "        # -------------------- DECODEUR --------------------\n",
        "        dec5_input = self._upsample_and_concat(enc4, bottleneck, self.up5)\n",
        "        dec5 = self.dec5(dec5_input)\n",
        "        out5 = self.out5(dec5)\n",
        "\n",
        "        dec6_input = self._upsample_and_concat(enc3, dec5, self.up6)\n",
        "        dec6 = self.dec6(dec6_input)\n",
        "        out6 = self.out6(dec6)\n",
        "\n",
        "        dec7_input = self._upsample_and_concat(enc2, dec6, self.up7)\n",
        "        dec7 = self.dec7(dec7_input)\n",
        "        out7 = self.out7(dec7)\n",
        "\n",
        "        dec8_input = self._upsample_and_concat(enc1, dec7, self.up8)\n",
        "        dec8 = self.dec8(dec8_input)\n",
        "\n",
        "        # -------------------- FUSION TEXTURE --------------------\n",
        "        # On s'assure que la texture est à la bonne taille\n",
        "        if extracted_texture.shape[-2:] != structure.shape[-2:]:\n",
        "            extracted_texture = F.interpolate(\n",
        "                extracted_texture,\n",
        "                size=structure.shape[-2:],\n",
        "                mode=\"bilinear\",\n",
        "                align_corners=False,\n",
        "            )\n",
        "\n",
        "        texture_fused = self.fuse_texture(\n",
        "            torch.cat([extracted_texture, dec8], dim=1)\n",
        "        )\n",
        "\n",
        "        # -------------------- FUSION FINALE --------------------\n",
        "        out5 = F.interpolate(\n",
        "            out5, size=structure.shape[-2:], mode=\"bilinear\", align_corners=False\n",
        "        )\n",
        "        out6 = F.interpolate(\n",
        "            out6, size=structure.shape[-2:], mode=\"bilinear\", align_corners=False\n",
        "        )\n",
        "        out7 = F.interpolate(\n",
        "            out7, size=structure.shape[-2:], mode=\"bilinear\", align_corners=False\n",
        "        )\n",
        "        texture_fused = F.interpolate(\n",
        "            texture_fused, size=structure.shape[-2:], mode=\"bilinear\", align_corners=False\n",
        "        )\n",
        "\n",
        "\n",
        "        final = self.final_fusion(torch.cat([out5, out6, out7, texture_fused], dim=1))\n",
        "\n",
        "        return final, out5, out6, out7\n",
        "\n",
        "\n",
        "class STDNetFullModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Modèle complet STD-Net:\n",
        "    - STDModule: décomposition I -> (S,T)\n",
        "    - TextureBlock: structure additionnelle extraite depuis T\n",
        "    - MNet: segmentation guidée par S et la texture extraite\n",
        "\n",
        "    forward(image):\n",
        "        -> seg_final, seg5, seg6, seg7, texture, extracted_structure, structure\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_channels: int = 3,\n",
        "        texture_channels: int = 3,\n",
        "        hidden_dim: int = 64,\n",
        "        img_size: int = 512,\n",
        "        num_classes: int = 1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.std_module = STDModule(img_channels=img_channels, hidden_dim=hidden_dim)\n",
        "        self.texture_block = TextureBlock(img_channels=texture_channels)\n",
        "        self.mnet = MNet(\n",
        "            img_size=img_size,\n",
        "            img_channels=img_channels,\n",
        "            texture_channels=texture_channels,\n",
        "            num_classes=num_classes,\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        image: torch.Tensor\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor,\n",
        "               torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        #On ressort la structure et la texture avec le module STD\n",
        "        structure, texture = self.std_module(image)\n",
        "        #On fait l'extracted_structure avec le textureblock\n",
        "        extracted_structure = self.texture_block(texture)\n",
        "        #On fait la segmentation avec la structure en entrée du MNEt ainsi que l'extracted_structure pour le dernier niveau\n",
        "        seg_final, seg5, seg6, seg7 = self.mnet(structure, extracted_structure)\n",
        "        #On retourne tout\n",
        "        return seg_final, seg5, seg6, seg7, texture, extracted_structure, structure\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkP8Z-Ca2kfD"
      },
      "outputs": [],
      "source": [
        "\n",
        "def structure_loss_tv(structure: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Total Variation Loss (TV) sur la composante de structure.\n",
        "    Encourage la structure à être régulière (peu de variations locales).\n",
        "    \"\"\"\n",
        "    # Différences absolues entre pixels voisins VERTICAUX :\n",
        "    # structure[:, :, 1:, :]  -> toutes les lignes sauf la première\n",
        "    # structure[:, :, :-1, :] -> toutes les lignes sauf la dernière\n",
        "    # dh a la forme [B, C, H-1, W] et mesure |S(i+1,j) - S(i,j)|\n",
        "    dh = torch.abs(structure[:, :, 1:, :] - structure[:, :, :-1, :])\n",
        "\n",
        "    # Différences absolues entre pixels voisins HORIZONTAUX :\n",
        "    # structure[:, :, :, 1:]  -> toutes les colonnes sauf la première\n",
        "    # structure[:, :, :, :-1] -> toutes les colonnes sauf la dernière\n",
        "    # dw a la forme [B, C, H, W-1] et mesure |S(i,j+1) - S(i,j)|\n",
        "    dw = torch.abs(structure[:, :, :, 1:] - structure[:, :, :, :-1])\n",
        "\n",
        "\n",
        "    return (dh.pow(2).mean() + dw.pow(2).mean()).sqrt()\n",
        "\n",
        "def texture_loss_l1(texture: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    L1 sur la texture (encourage la parcimonie = texture peu \"chargée\").\n",
        "    \"\"\"\n",
        "    # On prend la valeur absolue de tous les coefficients de texture\n",
        "    # et on en fait la moyenne : c'est ||texture||_1 normalisée.\n",
        "    return torch.mean(torch.abs(texture))\n",
        "\n",
        "\n",
        "def masked_bce_with_logits(\n",
        "    pred: torch.Tensor,\n",
        "    target: torch.Tensor,\n",
        "    fov: torch.Tensor,\n",
        "    bce_criterion: nn.Module,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    BCE avec logits restreint à la FOV (zone utile de la rétine).\n",
        "\n",
        "    Args:\n",
        "        pred:   [B,C,H,W], logits du modèle\n",
        "        target: [B,C,H,W], masque binaire (0/1)\n",
        "        fov:    [B,1,H,W], masque FOV (0/1) => où on calcule la loss\n",
        "    \"\"\"\n",
        "    # bce_criterion est une BCEWithLogitsLoss avec reduction='none'\n",
        "    # => loss_map a la même forme que pred : [B,C,H,W]\n",
        "    loss_map = bce_criterion(pred, target)  # [B,C,H,W]\n",
        "\n",
        "    # On met la FOV sous forme float 0/1 (si ce n'est pas déjà le cas)\n",
        "    mask = (fov > 0.5).float()  # [B,1,H,W]\n",
        "\n",
        "    # Si le prédicteur a plusieurs canaux (C>1) et la FOV n'en a qu'un (1),\n",
        "    # on duplique le masque sur tous les canaux pour pouvoir faire un produit élément par élément.\n",
        "    if mask.shape[1] != pred.shape[1]:\n",
        "        # expand(-1, C, -1, -1) -> garde B,H,W, mais remplace la dimension des canaux par C\n",
        "        mask = mask.expand(-1, pred.shape[1], -1, -1)  # [B,C,H,W]\n",
        "\n",
        "    # On ne garde la loss que dans les pixels à l'intérieur du FOV (mask=1),\n",
        "    # et on met la loss à 0 hors FOV (mask=0). -> Cela permet de ne pas prendre en compte le backg\n",
        "    loss = (loss_map * mask).sum() / (mask.sum() + 1e-8)\n",
        "    # → moyenne pondérée : somme des pertes sur FOV / nombre de pixels FOV\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def deep_supervision(\n",
        "    preds: Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor],\n",
        "    gt: torch.Tensor,\n",
        "    fov: torch.Tensor,\n",
        "    bce_criterion: nn.Module,\n",
        ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Applique la BCE sur les 4 cartes de sortie:\n",
        "        - seg_final (plein résolution)\n",
        "        - seg5, seg6, seg7 (résolutions intermédiaires)\n",
        "\n",
        "    Combine ensuite les 4 pertes en une seule Lseg (combinaison pondérée).\n",
        "    \"\"\"\n",
        "    seg_final, seg5, seg6, seg7 = preds\n",
        "\n",
        "    # On met la GT à la même taille que chaque sortie intermédiaire,\n",
        "    # en utilisant une interpolation 'nearest' (pas de valeurs intermédiaires, on garde 0/1).\n",
        "    gt5 = F.interpolate(gt, size=seg5.shape[-2:], mode=\"nearest\")\n",
        "    gt6 = F.interpolate(gt, size=seg6.shape[-2:], mode=\"nearest\")\n",
        "    gt7 = F.interpolate(gt, size=seg7.shape[-2:], mode=\"nearest\")\n",
        "\n",
        "    # Idem pour la FOV (on la redimensionne pour qu'elle matche chaque prédiction)\n",
        "    fov5 = F.interpolate(fov, size=seg5.shape[-2:], mode=\"nearest\")\n",
        "    fov6 = F.interpolate(fov, size=seg6.shape[-2:], mode=\"nearest\")\n",
        "    fov7 = F.interpolate(fov, size=seg7.shape[-2:], mode=\"nearest\")\n",
        "\n",
        "    # Calcul de la BCE masquée (sur la FOV) pour chaque niveau de résolution\n",
        "    L_final = masked_bce_with_logits(seg_final, gt,  fov,  bce_criterion)\n",
        "    L5      = masked_bce_with_logits(seg5,      gt5, fov5, bce_criterion)\n",
        "    L6      = masked_bce_with_logits(seg6,      gt6, fov6, bce_criterion)\n",
        "    L7      = masked_bce_with_logits(seg7,      gt7, fov7, bce_criterion)\n",
        "\n",
        "    # Poids pour chaque niveau\n",
        "    #Choisi de faire ceci pour prendre en compte les sorties inférieur du MNET mais qu'elles n'ai pas le meme poids que la sortie en pleine résolution.\n",
        "    weights = [1.0, 0.3, 0.1, 0.05]\n",
        "    sum_w = sum(weights)\n",
        "\n",
        "    # Combinaison pondérée des pertes :\n",
        "    Lseg = (\n",
        "        weights[0] * L_final\n",
        "        + weights[1] * L5\n",
        "        + weights[2] * L6\n",
        "        + weights[3] * L7\n",
        "    ) / sum_w\n",
        "\n",
        "    return Lseg, L_final, L5, L6, L7\n",
        "\n",
        "\n",
        "def total_loss(\n",
        "    seg_output: torch.Tensor,\n",
        "    seg5: torch.Tensor,\n",
        "    seg6: torch.Tensor,\n",
        "    seg7: torch.Tensor,\n",
        "    fov: torch.Tensor,\n",
        "    seg_target: torch.Tensor,\n",
        "    structure: torch.Tensor,\n",
        "    texture: torch.Tensor,\n",
        "    bce_criterion: nn.Module,\n",
        "    mu: float = 0.001,\n",
        "    lam: float = 1.0,\n",
        ") -> Dict[str, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Perte totale du papier :\n",
        "        Ltotal = Lseg + mu * (Lt + lam * Ls)\n",
        "\n",
        "    avec:\n",
        "        Lseg : loss de segmentation multi-échelle (deep supervision)\n",
        "        Lt   : L1 sur la texture\n",
        "        Ls   : TV sur la structure\n",
        "    \"\"\"\n",
        "    #  On calcule la loss de segmentation multi-échelle\n",
        "    Lseg, L_final, L5, L6, L7 = deep_supervision(\n",
        "        (seg_output, seg5, seg6, seg7), seg_target, fov, bce_criterion\n",
        "    )\n",
        "\n",
        "    #  Pertes de régularisation sur les décompositions structure / texture\n",
        "    Lt = texture_loss_l1(texture)      # encourage texture \"sparse\"\n",
        "    Ls = structure_loss_tv(structure)  # encourage structure \"lisse\"\n",
        "\n",
        "    #Combinaison finale :\n",
        "    #    - Lseg : terme principal (segmentation)\n",
        "    #    - mu   : poids global des pertes structure/texture\n",
        "    #    - lam  : réglage relatif entre Ls et Lt à l'intérieur du bloc mu*(Lt + lam*Ls)\n",
        "    Ltotal = Lseg + mu * (Lt + lam * Ls)\n",
        "\n",
        "    return {\n",
        "        \"total\": Ltotal,      # perte totale utilisée pour le backward\n",
        "        \"segmentation\": Lseg, # terme de segmentation (multi-scale)\n",
        "        \"texture\": Lt,        # régularisation L1 sur la texture\n",
        "        \"structure\": Ls,      # régularisation TV sur la structure\n",
        "        \"L_final\": L_final,   # perte juste sur la sortie finale\n",
        "        \"L5\": L5,\n",
        "        \"L6\": L6,\n",
        "        \"L7\": L7,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avjiTR_J2pNk"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# TRAIN / VAL / TEST LOOPS\n",
        "# =============================================================================\n",
        "\n",
        "def train_one_epoch(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    device: str,\n",
        "    bce_criterion: nn.Module\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Entraîne le modèle sur UNE epoch .\n",
        "    \"\"\"\n",
        "    # Passe le modèle en mode entraînement :\n",
        "    # - AdaptiveNorm met à jour ses stats\n",
        "    model.train()\n",
        "\n",
        "    # Accumulateurs pour le calcul des pertes\n",
        "    epoch_total_loss = 0.0  # perte totale (seg + texture + structure)\n",
        "    total_seg = 0.0         # partie segmentation\n",
        "    total_T = 0.0           # partie texture\n",
        "    total_S = 0.0           # partie structure\n",
        "\n",
        "    # Boucle sur tous les batchs du DataLoader passer en paramètre\n",
        "    for batch in tqdm(loader, desc=\"Train\"):\n",
        "        # On récupère les tenseurs et on les envoie sur le device processeur ou GPU\n",
        "        img = batch[\"image\"].to(device)\n",
        "        fov = batch[\"fov\"].to(device)\n",
        "        gt = batch[\"gt\"].to(device)\n",
        "\n",
        "        # On remet les gradients de l'optimizer à zéro avant le backward\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward : le modèle renvoie les cartes de segmentation + structure/texture\n",
        "        seg, seg5, seg6, seg7, texture, extracted_structure, structure = model(img)\n",
        "\n",
        "        # Calcul de toutes les composantes de la loss\n",
        "        losses = total_loss(\n",
        "            seg_output=seg,\n",
        "            seg5=seg5,\n",
        "            seg6=seg6,\n",
        "            seg7=seg7,\n",
        "            fov=fov,\n",
        "            seg_target=gt,\n",
        "            structure=structure,\n",
        "            texture=texture,\n",
        "            bce_criterion=bce_criterion,\n",
        "        )\n",
        "\n",
        "        # Perte totale, on va faire la backpropagation sur celle ci\n",
        "        loss = losses[\"total\"]\n",
        "        loss.backward()          # calcul des gradients\n",
        "        optimizer.step()         # mise à jour des poids du modèle passer en paramètre\n",
        "\n",
        "        # Accumulation pour les stats de fin d'epoch\n",
        "        epoch_total_loss += loss.item()\n",
        "        total_seg += losses[\"segmentation\"].item()\n",
        "        total_T += losses[\"texture\"].item()\n",
        "        total_S += losses[\"structure\"].item()\n",
        "\n",
        "    # Moyenne sur le nombre de batchs\n",
        "    n = len(loader)\n",
        "    return {\n",
        "        \"loss\": epoch_total_loss / n,\n",
        "        \"seg\": total_seg / n,\n",
        "        \"T\": total_T / n,\n",
        "        \"S\": total_S / n,\n",
        "    }\n",
        "\n",
        "#-> On désactive le calcul des gradients\n",
        "@torch.no_grad()\n",
        "def validate_one_epoch(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    device: str,\n",
        "    bce_criterion: nn.Module,\n",
        "    debug_visualization: bool = False,\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Évalue le modèle sur la validation .\n",
        "    On fait la même chose que train_one_epoch sans backpropagation.\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    epoch_total_loss = 0.0\n",
        "    total_seg = 0.0\n",
        "    total_T = 0.0\n",
        "    total_S = 0.0\n",
        "\n",
        "\n",
        "    for batch_idx, batch in enumerate(tqdm(loader, desc=\"Val\")):\n",
        "        img = batch[\"image\"].to(device)\n",
        "        fov = batch[\"fov\"].to(device)\n",
        "        gt = batch[\"gt\"].to(device)\n",
        "\n",
        "\n",
        "        seg, seg5, seg6, seg7, texture, extracted_structure, structure = model(img)\n",
        "\n",
        "        losses = total_loss(\n",
        "            seg_output=seg,\n",
        "            seg5=seg5,\n",
        "            seg6=seg6,\n",
        "            seg7=seg7,\n",
        "            fov=fov,\n",
        "            seg_target=gt,\n",
        "            structure=structure,\n",
        "            texture=texture,\n",
        "            bce_criterion=bce_criterion,\n",
        "        )\n",
        "\n",
        "\n",
        "        epoch_total_loss += losses[\"total\"].item()\n",
        "        total_seg += losses[\"segmentation\"].item()\n",
        "        total_T += losses[\"texture\"].item()\n",
        "        total_S += losses[\"structure\"].item()\n",
        "\n",
        "\n",
        "    n = len(loader)\n",
        "    return {\n",
        "        \"loss\": epoch_total_loss / n,\n",
        "        \"seg\": total_seg / n,\n",
        "        \"T\": total_T / n,\n",
        "        \"S\": total_S / n,\n",
        "    }\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test_drive_model(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    device: str,\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Évaluation sur DRIVE  avec :\n",
        "\n",
        "        - Acc : accuracy globale\n",
        "        - AUC : aire sous la courbe ROC\n",
        "        - Sen : sensibilité (rappel)\n",
        "        - Spe : spécificité\n",
        "        - IOU : Intersection over Union\n",
        "\n",
        "    Toutes les métriques sont calculées dans le FOV -> on ne prends pas en compte le tour du fond d'oeil\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Compteurs globaux pour la matrice de confusion\n",
        "    TP_total, FP_total, FN_total, TN_total = 0, 0, 0, 0\n",
        "\n",
        "    # Listes pour calculer l'AUC après coup\n",
        "    all_probs = []\n",
        "    all_targets = []\n",
        "    all_fovs = []\n",
        "\n",
        "    for batch in tqdm(loader, desc=\"Test DRIVE\"):\n",
        "        img = batch[\"image\"].to(device)\n",
        "        fov = batch[\"fov\"].to(device)\n",
        "        gt = batch[\"gt\"].to(device).float()\n",
        "\n",
        "        # Forward : on ne s'intéresse qu'à la sortie de segmentation pour l'évaluation totale du modèle et voir s'il arrive a généraliser sur des données nouvelles\n",
        "        seg, *_ = model(img)\n",
        "        prob = torch.sigmoid(seg)\n",
        "\n",
        "        # Masquage par le FOV -> On ne prends pas en compte le tour du fond d'oeil\n",
        "        prob_masked = prob * fov\n",
        "        target_masked = gt * fov\n",
        "\n",
        "        # Binarisation (seuil 0.5) pour calculer TP, FP, (Comme dit plus tot, avec la normalisation, on a des valeurs continue entre 0 et 1, ici c'est soit 1 soit 0 pas autre chose)\n",
        "        pred = (prob_masked > 0.5).float()\n",
        "\n",
        "        # TP : préd=1 et GT=1\n",
        "        TP = ((pred == 1) & (target_masked == 1)).sum().item()\n",
        "        # FP : préd=1, GT=0, mais uniquement dans la FOV\n",
        "        FP = ((pred == 1) & (target_masked == 0) & (fov == 1)).sum().item()\n",
        "        # FN : préd=0, GT=1\n",
        "        FN = ((pred == 0) & (target_masked == 1)).sum().item()\n",
        "        # TN : préd=0, GT=0, dans la FOV\n",
        "        TN = ((pred == 0) & (target_masked == 0) & (fov == 1)).sum().item()\n",
        "\n",
        "        TP_total += TP\n",
        "        FP_total += FP\n",
        "        FN_total += FN\n",
        "        TN_total += TN\n",
        "\n",
        "        # Pour l'AUC : on stocke toutes les probas + GT + FOV,\n",
        "        # on filtrera plus tard pour ne garder que les pixels dans la FOV.\n",
        "        all_probs.append(prob.detach().cpu().numpy().ravel())\n",
        "        all_targets.append(gt.detach().cpu().numpy().ravel())\n",
        "        all_fovs.append(fov.detach().cpu().numpy().ravel())\n",
        "\n",
        "    # ---- Métriques dérivées de la matrice de confusion ----\n",
        "    denominator = TP_total + TN_total + FP_total + FN_total + 1e-8\n",
        "    Acc = (TP_total + TN_total) / denominator                # Accuracy\n",
        "    Sen = TP_total / (TP_total + FN_total + 1e-8)            # Sensibilité (Recall)\n",
        "    Spe = TN_total / (TN_total + FP_total + 1e-8)            # Spécificité\n",
        "    IOU = TP_total / (TP_total + FP_total + FN_total + 1e-8) # Intersection over Union\n",
        "\n",
        "    # ---- AUC (ROC) en ne gardant que les pixels dans la FOV ----\n",
        "    probs_flat = np.concatenate(all_probs, axis=0)\n",
        "    targets_flat = np.concatenate(all_targets, axis=0)\n",
        "    fov_flat = np.concatenate(all_fovs, axis=0)\n",
        "\n",
        "    # On ne garde que les pixels où FOV=1\n",
        "    valid = (fov_flat == 1)\n",
        "    probs_valid = probs_flat[valid]\n",
        "    targets_valid = targets_flat[valid]\n",
        "\n",
        "    # Si toutes les cibles sont identiques (0 ou 1), l'AUC n'est pas définie\n",
        "    unique_classes = np.unique(targets_valid)\n",
        "    if unique_classes.size < 2:\n",
        "        AUC = float(\"nan\")\n",
        "    else:\n",
        "        AUC = roc_auc_score(targets_valid, probs_valid)\n",
        "\n",
        "    # Affichage des métriques\n",
        "    print(\"\\n--- DRIVE metrics (threshold=0.5) ---\")\n",
        "    print(f\"Acc : {Acc:.4f}\")\n",
        "    print(f\"AUC : {AUC:.4f}\")\n",
        "    print(f\"Sen : {Sen:.4f}\")\n",
        "    print(f\"Spe : {Spe:.4f}\")\n",
        "    print(f\"IOU : {IOU:.4f}\")\n",
        "\n",
        "    return {\"Acc\": Acc, \"AUC\": AUC, \"Sen\": Sen, \"Spe\": Spe, \"IOU\": IOU}\n",
        "\n",
        "\n",
        "def overlapping_error(pred_mask: torch.Tensor, gt_mask: torch.Tensor, eps: float = 1e-8) -> float:\n",
        "    \"\"\"\n",
        "    Overlapping Error (OE) = 1 - IoU.\n",
        "\n",
        "    pred_mask, gt_mask: Tensors [H,W] ou [1,H,W] (0/1 ou probas).\n",
        "    \"\"\"\n",
        "    # Si [1,H,W], on enlève la dimension canal pour travailler en [H,W]\n",
        "    if pred_mask.ndim == 3:\n",
        "        pred_mask = pred_mask[0]\n",
        "    if gt_mask.ndim == 3:\n",
        "        gt_mask = gt_mask[0]\n",
        "\n",
        "    # Binarisation des masques avec seuil 0.5\n",
        "    pred_bin = (pred_mask > 0.5).float()\n",
        "    gt_bin = (gt_mask > 0.5).float()\n",
        "\n",
        "    # Intersection = pixels où préd=1 et GT=1\n",
        "    inter = (pred_bin * gt_bin).sum().item()\n",
        "    # Union = pixels où préd=1 OU GT=1\n",
        "    union = ((pred_bin + gt_bin) > 0).float().sum().item()\n",
        "\n",
        "    # Cas dégénéré : aucune union, on retourne 0 (pas d'erreur d'overlap)\n",
        "    if union == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # IoU = |A∩B| / |A∪B|\n",
        "    iou = inter / (union + eps)\n",
        "    # OE = 1 - IoU (plus c'est proche de 0, mieux c'est)\n",
        "    return 1.0 - iou\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def compute_oe_origa(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    device: str,\n",
        "    desc: str = \"OE ORIGA\",\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Calcule l'Overlapping Error pour ORIGA sur :\n",
        "        - disque (canal 0)\n",
        "        - cup   (canal 1)\n",
        "        - total (somme des deux)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    oes_disc = []  # liste des OE disque sur tous les exemples\n",
        "    oes_cup = []   # liste des OE cup sur tous les exemples\n",
        "\n",
        "    for batch in tqdm(loader, desc=desc):\n",
        "        img = batch[\"image\"].to(device)\n",
        "        gt = batch[\"gt\"].to(device).float()  #(canal 0 = disc, 1 = cup)\n",
        "\n",
        "        # Forward\n",
        "        seg, *_ = model(img)\n",
        "        prob = torch.sigmoid(seg)  # probas [0,1]\n",
        "\n",
        "        # On calcule l'OE par image et par canal\n",
        "        for b in range(img.size(0)):\n",
        "            # Canal 0 = disque\n",
        "            oe_d = overlapping_error(prob[b, 0].cpu(), gt[b, 0].cpu())\n",
        "            # Canal 1 = cup\n",
        "            oe_c = overlapping_error(prob[b, 1].cpu(), gt[b, 1].cpu())\n",
        "            oes_disc.append(oe_d)\n",
        "            oes_cup.append(oe_c)\n",
        "\n",
        "    # Moyenne sur tout le dataset\n",
        "    OE_disc = float(np.mean(oes_disc))\n",
        "    OE_cup = float(np.mean(oes_cup))\n",
        "    OE_total = OE_disc + OE_cup  # métrique globale\n",
        "\n",
        "    return {\"disc\": OE_disc, \"cup\": OE_cup, \"total\": OE_total}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXOVCGP3glI-"
      },
      "outputs": [],
      "source": [
        "def log_epoch_stats(history, train_stats, val_stats, epoch, total_epochs, tag=\"DRIVE\"):\n",
        "    \"\"\"\n",
        "    Met à jour le dictionnaire history avec les stats train/val\n",
        "    et affiche un résumé lisible pour l'epoch courante.\n",
        "\n",
        "    Args:\n",
        "        history      : dict avec les listes d'historique\n",
        "        train_stats  : dict {\"loss\", \"seg\", \"T\", \"S\"} pour le train\n",
        "        val_stats    : dict {\"loss\", \"seg\", \"T\", \"S\"} pour la val\n",
        "        epoch        : numéro d'epoch (1-based)\n",
        "        total_epochs : nombre total d'epochs\n",
        "        tag          : nom du dataset / expérience (ex: \"DRIVE\", \"ORIGA\")\n",
        "    \"\"\"\n",
        "\n",
        "    # --- Mise à jour de l'historique ---\n",
        "    history[\"train_loss\"].append(train_stats[\"loss\"])\n",
        "    history[\"val_loss\"].append(val_stats[\"loss\"])\n",
        "\n",
        "    history[\"train_seg\"].append(train_stats[\"seg\"])\n",
        "    history[\"val_seg\"].append(val_stats[\"seg\"])\n",
        "\n",
        "    history[\"train_T\"].append(train_stats[\"T\"])\n",
        "    history[\"val_T\"].append(val_stats[\"T\"])\n",
        "\n",
        "    history[\"train_S\"].append(train_stats[\"S\"])\n",
        "    history[\"val_S\"].append(val_stats[\"S\"])\n",
        "\n",
        "    # --- Affichage propre ---\n",
        "    print(\n",
        "        f\"[{tag}][Epoch {epoch}/{total_epochs}] \"\n",
        "        f\"\\n[Train] Total loss={train_stats['loss']:.4f}, \"\n",
        "        f\" Segmentation={train_stats['seg']:.4f}, \"\n",
        "        f\" Texture={train_stats['T']:.4f}, \"\n",
        "        f\" Structure={train_stats['S']:.4f},\\n\"\n",
        "\n",
        "        f\"[Valid] Total loss={val_stats['loss']:.4f}, \"\n",
        "\n",
        "        f\" Segmentation={val_stats['seg']:.4f}, \"\n",
        "\n",
        "        f\" Texture={val_stats['T']:.4f}, \"\n",
        "\n",
        "        f\" Structure={val_stats['S']:.4f}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gp7l3MW_2tsv",
        "outputId": "374c1c64-54a2-4bd4-f635-45a89d9a0441"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------------------\n",
        "# DRIVE\n",
        "# -------------------------------------------------------------------------\n",
        "# 1) Définition des chemins\n",
        "drive_train_img_dir = \"/content/drive/MyDrive/Colab Notebooks/Traitement-image/data/DRIVE/training/images\"\n",
        "drive_train_fov_dir = \"/content/drive/MyDrive/Colab Notebooks/Traitement-image/data/DRIVE/training/mask\"\n",
        "drive_train_gt_dir = \"/content/drive/MyDrive/Colab Notebooks/Traitement-image/data/DRIVE/training/1st_manual\"\n",
        "\n",
        "drive_test_img_dir = \"/content/drive/MyDrive/Colab Notebooks/Traitement-image/data/DRIVE/test/images\"\n",
        "drive_test_fov_dir = \"/content/drive/MyDrive/Colab Notebooks/Traitement-image/data/DRIVE/test/mask\"\n",
        "drive_test_gt_dir = \"/content/drive/MyDrive/Colab Notebooks/Traitement-image/data/DRIVE/test/1st_manual\"\n",
        "drive_test_gt2_dir = \"/content/drive/MyDrive/Colab Notebooks/Traitement-image/data/DRIVE/test/2nd_manual\"\n",
        "\n",
        "image_size_drive = (512, 512)\n",
        "from typing import Tuple\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "\n",
        "def setup_drive_dataloaders(\n",
        "    image_size: Tuple[int, int] = (512, 512),\n",
        "    batch_size_train: int = 2,\n",
        "    batch_size_val: int = 2,\n",
        "    batch_size_test: int = 1,\n",
        "    num_workers: int = 2,\n",
        "    val_ratio: float = 0.2,\n",
        "    seed: int = 42,\n",
        "    apply_clahe_flag: bool = True,\n",
        "):\n",
        "    \"\"\"\n",
        "    Prépare tous les éléments pour DRIVE en une seule fonction :\n",
        "\n",
        "      - calcule mean/std sur le train\n",
        "      - crée les transforms train / eval\n",
        "      - crée les datasets train / val / test\n",
        "      - crée les DataLoaders correspondants\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Dataset sans transform pour calculer mean/std\n",
        "    drive_dataset_for_stats = DRIVEDataset(\n",
        "        images_dir=drive_train_img_dir,\n",
        "        fov_dir=drive_train_fov_dir,\n",
        "        gt_dir=drive_train_gt_dir,\n",
        "        split=\"train\",\n",
        "        transform=None,\n",
        "        apply_clahe_flag=apply_clahe_flag,\n",
        "    )\n",
        "\n",
        "    mean_drive, std_drive = compute_mean_std(\n",
        "        drive_dataset_for_stats,\n",
        "        image_key=\"image\",\n",
        "        image_size=image_size\n",
        "    )\n",
        "\n",
        "    # Transforms train / val / test\n",
        "    drive_train_transform = RetinalSegmentationTransform(\n",
        "        image_size=image_size,\n",
        "        mean=mean_drive,\n",
        "        std=std_drive,\n",
        "        train=True,\n",
        "        use_polar=False,\n",
        "    )\n",
        "    drive_eval_transform = RetinalSegmentationTransform(\n",
        "        image_size=image_size,\n",
        "        mean=mean_drive,\n",
        "        std=std_drive,\n",
        "        train=False,\n",
        "        use_polar=False,\n",
        "    )\n",
        "\n",
        "    # Datasets TRAIN / VAL\n",
        "    full_drive_train = DRIVEDataset(\n",
        "        images_dir=drive_train_img_dir,\n",
        "        fov_dir=drive_train_fov_dir,\n",
        "        gt_dir=drive_train_gt_dir,\n",
        "        split=\"train\",\n",
        "        transform=drive_train_transform,\n",
        "        apply_clahe_flag=apply_clahe_flag,\n",
        "    )\n",
        "\n",
        "    # Split train/val (par défaut 80/20)\n",
        "    n_train = len(full_drive_train)\n",
        "    indices = list(range(n_train))\n",
        "    np.random.seed(seed)\n",
        "    np.random.shuffle(indices)\n",
        "    split_idx = int((1.0 - val_ratio) * n_train)\n",
        "    train_indices = indices[:split_idx]\n",
        "    val_indices = indices[split_idx:]\n",
        "\n",
        "    drive_train_dataset = Subset(full_drive_train, train_indices)\n",
        "\n",
        "    full_drive_val = DRIVEDataset(\n",
        "        images_dir=drive_train_img_dir,\n",
        "        fov_dir=drive_train_fov_dir,\n",
        "        gt_dir=drive_train_gt_dir,\n",
        "        split=\"train\",\n",
        "        transform=drive_eval_transform,\n",
        "        apply_clahe_flag=apply_clahe_flag,\n",
        "    )\n",
        "    drive_val_dataset = Subset(full_drive_val, val_indices)\n",
        "\n",
        "    drive_train_loader = DataLoader(\n",
        "        drive_train_dataset,\n",
        "        batch_size=batch_size_train,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "    )\n",
        "    drive_val_loader = DataLoader(\n",
        "        drive_val_dataset,\n",
        "        batch_size=batch_size_val,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "    )\n",
        "\n",
        "    # 4) Test set\n",
        "    drive_test_dataset = DRIVEDataset(\n",
        "        images_dir=drive_test_img_dir,\n",
        "        fov_dir=drive_test_fov_dir,\n",
        "        gt_dir=drive_test_gt_dir,\n",
        "        second_gt_dir=drive_test_gt2_dir,\n",
        "        split=\"test\",\n",
        "        transform=drive_eval_transform,\n",
        "        apply_clahe_flag=apply_clahe_flag,\n",
        "    )\n",
        "    drive_test_loader = DataLoader(\n",
        "        drive_test_dataset,\n",
        "        batch_size=batch_size_test,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "    )\n",
        "\n",
        "\n",
        "    return drive_train_loader, drive_val_loader, drive_test_loader, mean_drive, std_drive\n",
        "\n",
        "drive_train_loader, drive_val_loader, drive_test_loader, mean_drive, std_drive = setup_drive_dataloaders()\n",
        "\n",
        "\n",
        "# Modèle + optim + loss\n",
        "drive_model = STDNetFullModel(\n",
        "    img_channels=3,\n",
        "    texture_channels=3,\n",
        "    hidden_dim=64,\n",
        "    img_size=image_size_drive[0],\n",
        ").to(device)\n",
        "\n",
        "optimizer_drive = torch.optim.Adam(drive_model.parameters(), lr=1e-3)\n",
        "bce_criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
        "\n",
        "#Scheduler : réduit le LR quand la loss de validation ne diminue plus\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer_drive,\n",
        "    mode=\"min\",      # on veut MINIMISER la loss de validation\n",
        "    factor=0.5,      # LR_new = LR_old * 0.5\n",
        "    patience=10,\n",
        ")\n",
        "\n",
        "# Pour sauvegarder le meilleur modèle\n",
        "best_val_loss = float(\"inf\")\n",
        "best_epoch = 0\n",
        "best_model_path = \"stdnet_drive_best.pth\"\n",
        "\n",
        "# Entraînement\n",
        "EPOCHS_DRIVE = 150\n",
        "\n",
        "history_drive = {\n",
        "    \"train_loss\": [],\n",
        "    \"val_loss\": [],\n",
        "    \"train_seg\": [],\n",
        "    \"val_seg\": [],\n",
        "    \"train_T\":   [],\n",
        "    \"val_T\":     [],\n",
        "    \"train_S\":   [],\n",
        "    \"val_S\":     [],\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "#Lancement de l'entrainement\n",
        "for epoch in range(1, EPOCHS_DRIVE + 1):\n",
        "\n",
        "#Appel à la fonction qui train sur une epoch\n",
        "    train_stats = train_one_epoch(\n",
        "        drive_model,\n",
        "        drive_train_loader,\n",
        "        optimizer_drive,\n",
        "        device,\n",
        "        bce_criterion,\n",
        "    )\n",
        "#Validation sur l'epoch\n",
        "    val_stats = validate_one_epoch(\n",
        "        drive_model,\n",
        "        drive_val_loader,\n",
        "        device,\n",
        "        bce_criterion,\n",
        "    )\n",
        "    #Affectation de la métrique à surveiller pour baisser le learning rate si ca stagne\n",
        "    scheduler.step(val_stats[\"loss\"])\n",
        "\n",
        "    #Sauvegarde du meilleur modèle sur la meilleure loss validation\n",
        "    if val_stats[\"loss\"] < best_val_loss:\n",
        "        best_val_loss = val_stats[\"loss\"]\n",
        "        best_epoch = epoch\n",
        "        torch.save(drive_model.state_dict(), best_model_path)\n",
        "        print(f\"==> Nouveau meilleur modèle (epoch {epoch})\")\n",
        "\n",
        "#Affichage des loss (totale,seg,texture et structure)\n",
        "    log_epoch_stats(\n",
        "        history=history_drive,\n",
        "        train_stats=train_stats,\n",
        "        val_stats=val_stats,\n",
        "        epoch=epoch,\n",
        "        total_epochs=EPOCHS_DRIVE,\n",
        "        tag=\"DRIVE\"\n",
        "    )\n",
        "\n",
        "#Affichage du premier sample de la validation\n",
        "    show_drive_val_sample(\n",
        "        model=drive_model,\n",
        "        val_loader=drive_val_loader,\n",
        "        device=device,\n",
        "        title_prefix=f\"DRIVE - epoch {epoch}\",\n",
        "        threshold=0.5, #-> Pour avoir la segmentation en binaire, j e mets le threshold à 0.5\n",
        "    )\n",
        "\n",
        "#On charge le meilleure modèle sauvegarder\n",
        "best_model = STDNetFullModel(\n",
        "    img_channels=3,\n",
        "    texture_channels=3,\n",
        "    hidden_dim=64,\n",
        "    img_size=image_size_drive[0],\n",
        ").to(device)\n",
        "\n",
        "best_model.load_state_dict(torch.load(best_model_path))\n",
        "\n",
        "\n",
        "# et on l'évalue\n",
        "drive_metrics = test_drive_model(best_model, drive_test_loader, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "N70Twl5dIvfb",
        "outputId": "6a07af0d-117f-461a-de5b-c99e48cee675"
      },
      "outputs": [],
      "source": [
        "# ================== COURBES D'ENTRAÎNEMENT ==================\n",
        "epochs_range = range(1, EPOCHS_DRIVE + 1)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs_range, history_drive[\"train_loss\"], label=\"Train total loss\")\n",
        "plt.plot(epochs_range, history_drive[\"val_loss\"],   label=\"Val total loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Courbe de perte totale\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs_range, history_drive[\"train_seg\"], label=\"Train seg loss\")\n",
        "plt.plot(epochs_range, history_drive[\"val_seg\"],   label=\"Val seg loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Segmentation loss\")\n",
        "plt.title(\"Perte de segmentation (deep supervision)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs_range, history_drive[\"train_T\"], label=\"Train texture loss\")\n",
        "plt.plot(epochs_range, history_drive[\"val_T\"],   label=\"Val texture loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Texture loss\")\n",
        "plt.title(\"Perte texture Lt\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs_range, history_drive[\"train_S\"], label=\"Train structure loss\")\n",
        "plt.plot(epochs_range, history_drive[\"val_S\"],   label=\"Val structure loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Structure loss\")\n",
        "plt.title(\"Perte structure Ls\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfxZ4lyz2vub"
      },
      "outputs": [],
      "source": [
        "origa_csv = \"/content/drive/MyDrive/Colab Notebooks/Traitement-image/data/ORIGA/OrigaList.csv\"\n",
        "origa_img_dir = \"/content/drive/MyDrive/Colab Notebooks/Traitement-image/data/ORIGA/Images_Cropped\"\n",
        "origa_mask_dir = \"/content/drive/MyDrive/Colab Notebooks/Traitement-image/data/ORIGA/Masks_Cropped\"\n",
        "image_size_origa = (256, 256)\n",
        "\n",
        "\n",
        "def show_origa_val_sample(\n",
        "    model,\n",
        "    val_loader,\n",
        "    device,\n",
        "    title_prefix=\"ORIGA - epoch\",\n",
        "    threshold: float | None = 0.5,\n",
        "):\n",
        "    \"\"\"\n",
        "    Affiche, pour 1 image du set de validation ORIGA :\n",
        "      - structure S\n",
        "      - texture |T|\n",
        "      - extracted structure (canal vert uniquement)\n",
        "      - segmentation disque\n",
        "      - segmentation cup\n",
        "\n",
        "    On prend simplement le premier batch du loader.\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # On récupère UN batch de validation\n",
        "    batch = next(iter(val_loader))\n",
        "    img  = batch[\"image\"].to(device)\n",
        "    gt   = batch[\"gt\"].to(device)\n",
        "    fov  = batch.get(\"fov\", None)\n",
        "\n",
        "    if fov is not None:\n",
        "        fov = fov.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        seg, _, _, _, texture, extracted_structure, structure = model(img)\n",
        "        prob = torch.sigmoid(seg)\n",
        "\n",
        "\n",
        "    S  = structure[0].cpu()\n",
        "    T  = texture[0].cpu()\n",
        "    E  = extracted_structure[0].cpu()\n",
        "\n",
        "\n",
        "    P_disc = prob[0, 0].cpu()\n",
        "    P_cup  = prob[0, 1].cpu()\n",
        "\n",
        "    H, W = P_disc.shape\n",
        "\n",
        "\n",
        "    if fov is not None:\n",
        "        F = fov[0, 0].cpu()\n",
        "    else:\n",
        "        F = torch.ones_like(P_disc)\n",
        "\n",
        "    # -------Structure S (normalisée) ----------\n",
        "    S_vis = S.clone()\n",
        "    s_min, s_max = S_vis.min(), S_vis.max()\n",
        "    S_vis = (S_vis - s_min) / (s_max - s_min + 1e-8)\n",
        "    S_vis = S_vis * F.unsqueeze(0)     # [3,H,W]\n",
        "    S_vis = S_vis.permute(1, 2, 0)     # [H,W,3]\n",
        "\n",
        "    # ---------- Texture |T| (moyenne des canaux) ----------\n",
        "    T_abs = T.abs().mean(0)\n",
        "    t_min, t_max = T_abs.min(), T_abs.max()\n",
        "    T_vis = (T_abs - t_min) / (t_max - t_min + 1e-8)\n",
        "    T_vis = T_vis * F\n",
        "\n",
        "    # ----------  Extracted structure – canal vert uniquement ----------\n",
        "    E_green = E[1]\n",
        "    e_min, e_max = E_green.min(), E_green.max()\n",
        "    E_vis = (E_green - e_min) / (e_max - e_min + 1e-8)\n",
        "    E_vis = E_vis * F\n",
        "\n",
        "    # ---------- Segmentation disque & cup ----------\n",
        "    if threshold is None:\n",
        "        disc_mask = P_disc * F\n",
        "        cup_mask  = P_cup  * F\n",
        "    else:\n",
        "        disc_mask = (P_disc >= threshold).float() * F\n",
        "        cup_mask  = (P_cup  >= threshold).float() * F\n",
        "\n",
        "\n",
        "    # ---------- AFFICHAGE ----------\n",
        "    plt.figure(figsize=(18, 4))\n",
        "\n",
        "    # Structure S\n",
        "    plt.subplot(1, 5, 1)\n",
        "    plt.imshow(S_vis.numpy())\n",
        "    plt.title(f\"{title_prefix} - Structure\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    # 2) Texture |T|\n",
        "    plt.subplot(1, 5, 2)\n",
        "    plt.imshow(T_vis.numpy(), cmap=\"gray\")\n",
        "    plt.title(\"Texture\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    # 3) Structure extraite (canal vert)\n",
        "    plt.subplot(1, 5, 3)\n",
        "    plt.imshow(E_vis.numpy(), cmap=\"gray\")\n",
        "    plt.title(\"Extracted structure (G)\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    # 4) Segmentation Disque\n",
        "    plt.subplot(1, 5, 4)\n",
        "    plt.imshow(disc_mask.numpy(), cmap=\"gray\", vmin=0, vmax=1)\n",
        "    plt.title(\"Disc mask\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    # 5) Segmentation Cup\n",
        "    plt.subplot(1, 5, 5)\n",
        "    plt.imshow(cup_mask.numpy(), cmap=\"gray\", vmin=0, vmax=1)\n",
        "    plt.title(\"Cup mask\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def setup_origa_dataloaders(\n",
        "    image_size: Tuple[int, int] = (256, 256),\n",
        "    batch_size_train: int = 2,\n",
        "    batch_size_val: int = 2,\n",
        "    batch_size_test: int = 1,\n",
        "    num_workers: int = 2,\n",
        "    val_ratio: float = 0.2,\n",
        "    seed: int = 42,\n",
        "    rotation_deg: float = 15.0,\n",
        "):\n",
        "    \"\"\"\n",
        "    Prépare tous les éléments pour ORIGA en une seule fonction :\n",
        "\n",
        "      - calcule mean/std sur le train\n",
        "      - crée les transforms train / eval\n",
        "      - crée les datasets train / val / test\n",
        "      - crée les DataLoaders correspondants\n",
        "    \"\"\"\n",
        "\n",
        "    # 1) Dataset temporaire pour stats\n",
        "    origa_stats_dataset = ORIGADataset(\n",
        "        csv_path=origa_csv,\n",
        "        images_dir=origa_img_dir,\n",
        "        masks_dir=origa_mask_dir,\n",
        "        split=\"train\",\n",
        "        transform=None,                # pas de RetinalSegmentationTransform pour les stats\n",
        "        output_image_size=image_size,\n",
        "    )\n",
        "\n",
        "    mean_origa, std_origa = compute_mean_std(\n",
        "        origa_stats_dataset,\n",
        "        image_key=\"image\",\n",
        "        image_size=None\n",
        "    )\n",
        "\n",
        "    # 2) Transforms train / eval avec transformation polaire activée\n",
        "    origa_train_transform = RetinalSegmentationTransform(\n",
        "        image_size=image_size,\n",
        "        mean=mean_origa,\n",
        "        std=std_origa,\n",
        "        train=True,\n",
        "        use_polar=True,\n",
        "        rotation_deg=rotation_deg,\n",
        "    )\n",
        "    origa_eval_transform = RetinalSegmentationTransform(\n",
        "        image_size=image_size,\n",
        "        mean=mean_origa,\n",
        "        std=std_origa,\n",
        "        train=False,\n",
        "        use_polar=True,\n",
        "    )\n",
        "\n",
        "    # Dataset train complet (split officiel \"train\")\n",
        "    full_origa_train = ORIGADataset(\n",
        "        csv_path=origa_csv,\n",
        "        images_dir=origa_img_dir,\n",
        "        masks_dir=origa_mask_dir,\n",
        "        split=\"train\",\n",
        "        transform=origa_train_transform,\n",
        "        output_image_size=image_size,\n",
        "    )\n",
        "\n",
        "    # Split train/val (par défaut 80/20)\n",
        "    n_origa = len(full_origa_train)\n",
        "    indices = list(range(n_origa))\n",
        "    np.random.seed(seed)\n",
        "    np.random.shuffle(indices)\n",
        "    split_idx = int((1.0 - val_ratio) * n_origa)\n",
        "    origa_train_idx = indices[:split_idx]\n",
        "    origa_val_idx = indices[split_idx:]\n",
        "\n",
        "    origa_train_dataset = Subset(full_origa_train, origa_train_idx)\n",
        "\n",
        "    # Dataset val avec transform \"eval\"\n",
        "    full_origa_val = ORIGADataset(\n",
        "        csv_path=origa_csv,\n",
        "        images_dir=origa_img_dir,\n",
        "        masks_dir=origa_mask_dir,\n",
        "        split=\"train\",\n",
        "        transform=origa_eval_transform,\n",
        "        output_image_size=image_size,\n",
        "    )\n",
        "    origa_val_dataset = Subset(full_origa_val, origa_val_idx)\n",
        "\n",
        "    # 4) Test set\n",
        "    origa_test_dataset = ORIGADataset(\n",
        "        csv_path=origa_csv,\n",
        "        images_dir=origa_img_dir,\n",
        "        masks_dir=origa_mask_dir,\n",
        "        split=\"test\",\n",
        "        transform=origa_eval_transform,\n",
        "        output_image_size=image_size,\n",
        "    )\n",
        "\n",
        "    #  DataLoaders\n",
        "    origa_train_loader = DataLoader(\n",
        "        origa_train_dataset,\n",
        "        batch_size=batch_size_train,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "    )\n",
        "    origa_val_loader = DataLoader(\n",
        "        origa_val_dataset,\n",
        "        batch_size=batch_size_val,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "    )\n",
        "    origa_test_loader = DataLoader(\n",
        "        origa_test_dataset,\n",
        "        batch_size=batch_size_test,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "    )\n",
        "\n",
        "    # On renvoie les loaders + les stats de normalisation\n",
        "    return origa_train_loader, origa_val_loader, origa_test_loader, mean_origa, std_origa\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2_QEF3GHfza-",
        "outputId": "46d25a44-d2e7-4477-c834-0a20ba8de9cf"
      },
      "outputs": [],
      "source": [
        "origa_csv = \"/content/drive/MyDrive/Colab Notebooks/Traitement-image/data/ORIGA/OrigaList.csv\"\n",
        "origa_img_dir = \"/content/drive/MyDrive/Colab Notebooks/Traitement-image/data/ORIGA/Images_Cropped\"\n",
        "origa_mask_dir = \"/content/drive/MyDrive/Colab Notebooks/Traitement-image/data/ORIGA/Masks_Cropped\"\n",
        "image_size_origa = (256, 256)\n",
        "\n",
        "\n",
        "origa_train_loader, origa_val_loader, origa_test_loader, mean_origa, std_origa = setup_origa_dataloaders(\n",
        "    image_size=image_size_origa\n",
        ")\n",
        "\n",
        "# Modèle ORIGA : 2 canaux de sortie\n",
        "origa_model = STDNetFullModel(\n",
        "    img_channels=3,\n",
        "    texture_channels=3,\n",
        "    hidden_dim=64,\n",
        "    img_size=image_size_origa[0],\n",
        "    num_classes=2\n",
        ").to(device)\n",
        "\n",
        "#Optimiseur, loss, scheduler\n",
        "optimizer_origa = torch.optim.Adam(origa_model.parameters(), lr=1e-3)\n",
        "bce_criterion_origa = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
        "\n",
        "scheduler_origa = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer_origa,\n",
        "    mode=\"min\",\n",
        "    factor=0.5,\n",
        "    patience=10\n",
        "\n",
        ")\n",
        "\n",
        "#  Historique et suivi du meilleur modèle\n",
        "EPOCHS_ORIGA = 80\n",
        "\n",
        "history_origa = {\n",
        "    \"train_loss\": [],\n",
        "    \"val_loss\": [],\n",
        "    \"train_seg\": [],\n",
        "    \"val_seg\": [],\n",
        "    \"train_T\":   [],\n",
        "    \"val_T\":     [],\n",
        "    \"train_S\":   [],\n",
        "    \"val_S\":     [],\n",
        "}\n",
        "\n",
        "best_val_loss = float(\"inf\")\n",
        "best_epoch_origa = 0\n",
        "best_origa_model_path = \"stdnet_origa_best.pth\"\n",
        "\n",
        "#  Boucle d'entraînement\n",
        "for epoch in range(1, EPOCHS_ORIGA + 1):\n",
        "\n",
        "\n",
        "    train_stats = train_one_epoch(\n",
        "        origa_model,\n",
        "        origa_train_loader,\n",
        "        optimizer_origa,\n",
        "        device,\n",
        "        bce_criterion_origa,\n",
        "    )\n",
        "\n",
        "\n",
        "    val_stats = validate_one_epoch(\n",
        "        origa_model,\n",
        "        origa_val_loader,\n",
        "        device,\n",
        "        bce_criterion_origa,\n",
        "    )\n",
        "\n",
        "    # Mise à jour du scheduler\n",
        "    scheduler_origa.step(val_stats[\"loss\"])\n",
        "\n",
        "\n",
        "    log_epoch_stats(\n",
        "        history=history_origa,\n",
        "        train_stats=train_stats,\n",
        "        val_stats=val_stats,\n",
        "        epoch=epoch,\n",
        "        total_epochs=EPOCHS_ORIGA,\n",
        "        tag=\"ORIGA\"\n",
        "    )\n",
        "\n",
        "\n",
        "    if val_stats[\"loss\"] < best_val_loss:\n",
        "        best_val_loss = val_stats[\"loss\"]\n",
        "        best_epoch_origa = epoch\n",
        "        torch.save(origa_model.state_dict(), best_origa_model_path)\n",
        "        print(f\"==> Nouveau meilleur modèle ORIGA (epoch {epoch}, val_loss={best_val_loss:.4f})\")\n",
        "\n",
        "\n",
        "    show_origa_val_sample(\n",
        "        model=origa_model,\n",
        "        val_loader=origa_val_loader,\n",
        "        device=device,\n",
        "        title_prefix=f\"ORIGA - epoch {epoch}\",\n",
        "        threshold=0.5,  #Pour l'affichage\n",
        "    )\n",
        "\n",
        "#  Rechargement du meilleur modèle pour l'évaluation finale\n",
        "best_origa_model = STDNetFullModel(\n",
        "    img_channels=3,\n",
        "    texture_channels=3,\n",
        "    hidden_dim=64,\n",
        "    img_size=image_size_origa[0],\n",
        "    num_classes=2\n",
        ").to(device)\n",
        "\n",
        "best_origa_model.load_state_dict(torch.load(best_origa_model_path, map_location=device))\n",
        "\n",
        "\n",
        "#  Overlapping Error test avec le MEILLEUR modèle\n",
        "oe_test = compute_oe_origa(best_origa_model, origa_test_loader, device, desc=\"OE ORIGA TEST\")\n",
        "\n",
        "print(\"\\n===== OE ORIGA (disc + cup) =====\")\n",
        "print(f\"Test : OE_disc={oe_test['disc']:.4f}, OEcup={oe_test['cup']:.4f}, OEtotal={oe_test['total']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "ytMkyjBzf4RO",
        "outputId": "07e6577f-728a-4f4e-8aea-be59c0c8458d"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# ----- COURBES ORIGA -----\n",
        "epochs_origa = np.arange(1, len(history_origa[\"train_loss\"]) + 1)\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "#Loss totale\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(epochs_origa, history_origa[\"train_loss\"], label=\"train\")\n",
        "plt.plot(epochs_origa, history_origa[\"val_loss\"],   label=\"val\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Total loss\")\n",
        "plt.title(\"ORIGA - Total loss\")\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.3)\n",
        "\n",
        "#  Loss segmentation\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(epochs_origa, history_origa[\"train_seg\"], label=\"train\")\n",
        "plt.plot(epochs_origa, history_origa[\"val_seg\"],   label=\"val\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Segmentation loss\")\n",
        "plt.title(\"ORIGA - Segmentation\")\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.3)\n",
        "\n",
        "# Loss texture\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot(epochs_origa, history_origa[\"train_T\"], label=\"train\")\n",
        "plt.plot(epochs_origa, history_origa[\"val_T\"],   label=\"val\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Texture loss (Lt)\")\n",
        "plt.title(\"ORIGA - Texture\")\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.3)\n",
        "\n",
        "# Loss structure\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.plot(epochs_origa, history_origa[\"train_S\"], label=\"train\")\n",
        "plt.plot(epochs_origa, history_origa[\"val_S\"],   label=\"val\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Structure loss (Ls)\")\n",
        "plt.title(\"ORIGA - Structure\")\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4iH0mZGcjb3"
      },
      "outputs": [],
      "source": [
        "from typing import Callable, Optional, Tuple, Dict\n",
        "\n",
        "class REFUGEDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset REFUGE pour segmentation disque + cup.\n",
        "\n",
        "    Sorties :\n",
        "      - image : [3,H,W], float32 normalisée plus tard\n",
        "      - gt    : [2,H,W], 0/1 (canal 0 = disc, canal 1 = cup)\n",
        "      - fov   : [1,H,W], ici tout à 1 (pas de masque FOV fourni)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        images_dir: str,\n",
        "        masks_dir: str,\n",
        "        transform: Optional[Callable] = None,\n",
        "        disc_label_val: int = 1,\n",
        "        cup_label_val: int = 2,\n",
        "        image_extensions: Tuple[str, ...] = (\".jpg\", \".png\", \".jpeg\", \".bmp\", \".tif\", \".tiff\"),\n",
        "        mask_extensions: Tuple[str, ...] = (\".png\", \".bmp\", \".jpg\", \".jpeg\", \".tif\", \".tiff\"),\n",
        "        output_image_size: Optional[Tuple[int, int]] = None,\n",
        "    ):\n",
        "        self.images_dir = Path(images_dir)\n",
        "        self.masks_dir = Path(masks_dir)\n",
        "        self.transform = transform\n",
        "        self.disc_label_val = disc_label_val\n",
        "        self.cup_label_val = cup_label_val\n",
        "        self.image_extensions = image_extensions\n",
        "        self.mask_extensions = mask_extensions\n",
        "        self.output_image_size = output_image_size\n",
        "\n",
        "        if not self.images_dir.is_dir():\n",
        "            raise FileNotFoundError(f\"Images dir not found: {self.images_dir}\")\n",
        "        if not self.masks_dir.is_dir():\n",
        "            raise FileNotFoundError(f\"Masks dir not found: {self.masks_dir}\")\n",
        "\n",
        "        # On liste les fichiers image\n",
        "        self.image_files = sorted(\n",
        "            [f for f in os.listdir(self.images_dir) if f.lower().endswith(self.image_extensions)]\n",
        "        )\n",
        "        if len(self.image_files) == 0:\n",
        "            raise RuntimeError(f\"Aucune image trouvée dans {self.images_dir}\")\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def _find_mask_path(self, image_name: str) -> Path:\n",
        "        \"\"\"\n",
        "        On cherche un masque ayant le même nom de base que l'image,\n",
        "        mais avec une des extensions autorisées.\n",
        "        \"\"\"\n",
        "        stem = Path(image_name).stem\n",
        "        for ext in self.mask_extensions:\n",
        "            cand = self.masks_dir / f\"{stem}{ext}\"\n",
        "            if cand.is_file():\n",
        "                return cand\n",
        "        raise FileNotFoundError(f\"Aucun masque trouvé pour {image_name} dans {self.masks_dir}\")\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
        "        img_name = self.image_files[idx]\n",
        "        img_path = self.images_dir / img_name\n",
        "        mask_path = self._find_mask_path(img_name)\n",
        "\n",
        "        # --- Chargement image + masque ---\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        # img = apply_clahe(img)\n",
        "\n",
        "        # masque en niveaux de gris\n",
        "        mask = Image.open(mask_path).convert(\"L\")\n",
        "\n",
        "\n",
        "        if self.output_image_size is not None and self.transform is None:\n",
        "            img = img.resize(self.output_image_size, Image.BILINEAR)\n",
        "            mask = mask.resize(self.output_image_size, Image.NEAREST)\n",
        "\n",
        "        img_tensor = TF.to_tensor(img)  # [3,H,W]\n",
        "        mask_np = np.array(mask, dtype=np.float32)  # [H,W]\n",
        "\n",
        "        # On s'assure que les labels sont bien 0/1/2\n",
        "        if mask_np.max() > 2:\n",
        "            disc_region = (mask_np > 0) & (mask_np < 200)\n",
        "            cup_region = (mask_np >= 200)\n",
        "\n",
        "            labels = np.zeros_like(mask_np, dtype=np.float32)\n",
        "            labels[disc_region] = self.disc_label_val\n",
        "            labels[cup_region] = self.cup_label_val\n",
        "        else:\n",
        "            labels = mask_np  # déjà 0/1/2\n",
        "\n",
        "        mask_tensor = torch.from_numpy(labels).unsqueeze(0)  # [1,H,W]\n",
        "\n",
        "        masks = {\"mask\": mask_tensor}\n",
        "\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img_tensor, masks = self.transform(img_tensor, masks)\n",
        "\n",
        "        mask_tensor = masks[\"mask\"].squeeze(0)\n",
        "\n",
        "        # On construit les canaux disc/cup binaires\n",
        "        disc = (mask_tensor == float(self.disc_label_val)).float().unsqueeze(0)\n",
        "        cup = (mask_tensor == float(self.cup_label_val)).float().unsqueeze(0)\n",
        "        gt = torch.cat([disc, cup], dim=0)\n",
        "\n",
        "        # Pas de masque FOV fourni sur REFUGE → on prend tout\n",
        "        fov = torch.ones_like(disc)\n",
        "\n",
        "        return {\n",
        "            \"image\": img_tensor,\n",
        "            \"gt\": gt,\n",
        "            \"disc\": disc,\n",
        "            \"cup\": cup,\n",
        "            \"fov\": fov,\n",
        "            \"name\": img_name,\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2Ph8QkHcpNB"
      },
      "outputs": [],
      "source": [
        "def setup_refuge_dataloaders(\n",
        "    base_dir: str,\n",
        "    image_size: Tuple[int, int] = (256, 256),\n",
        "    batch_size_train: int = 2,\n",
        "    batch_size_val: int = 2,\n",
        "    batch_size_test: int = 1,\n",
        "    num_workers: int = 2,\n",
        "    apply_polar: bool = False,\n",
        "    seed: int = 42,\n",
        "):\n",
        "    \"\"\"\n",
        "    Prépare les DataLoaders pour REFUGE à partir d'un dossier de base :\n",
        "        base_dir/\n",
        "            train/Images\n",
        "            train/Masks\n",
        "            val/Images\n",
        "            val/Masks\n",
        "            test/Images\n",
        "            test/Masks\n",
        "\n",
        "    Retourne :\n",
        "        train_loader, val_loader, test_loader, mean_refuge, std_refuge\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    train_img_dir = os.path.join(base_dir, \"train\", \"Images_Cropped\")\n",
        "    train_mask_dir = os.path.join(base_dir, \"train\", \"Masks_Cropped\")\n",
        "    val_img_dir   = os.path.join(base_dir, \"val\",   \"Images_Cropped\")\n",
        "    val_mask_dir  = os.path.join(base_dir, \"val\",   \"Masks_Cropped\")\n",
        "    test_img_dir  = os.path.join(base_dir, \"test\",  \"Images_Cropped\")\n",
        "    test_mask_dir = os.path.join(base_dir, \"test\",  \"Masks_Cropped\")\n",
        "\n",
        "    # Dataset sans transform pour calculer mean/std\n",
        "    refuge_stats_dataset = REFUGEDataset(\n",
        "        images_dir=train_img_dir,\n",
        "        masks_dir=train_mask_dir,\n",
        "        transform=None,\n",
        "        output_image_size=image_size,\n",
        "    )\n",
        "\n",
        "    mean_refuge, std_refuge = compute_mean_std(\n",
        "        refuge_stats_dataset,\n",
        "        image_key=\"image\",\n",
        "        image_size=None,\n",
        "    )\n",
        "\n",
        "    # Transforms train / eval\n",
        "    refuge_train_transform = RetinalSegmentationTransform(\n",
        "        image_size=image_size,\n",
        "        mean=mean_refuge,\n",
        "        std=std_refuge,\n",
        "        train=True,\n",
        "        use_polar=apply_polar,\n",
        "        rotation_deg=5.0,\n",
        "    )\n",
        "    refuge_eval_transform = RetinalSegmentationTransform(\n",
        "        image_size=image_size,\n",
        "        mean=mean_refuge,\n",
        "        std=std_refuge,\n",
        "        train=False,\n",
        "        use_polar=apply_polar,\n",
        "    )\n",
        "\n",
        "    #  Datasets\n",
        "    refuge_train_dataset = REFUGEDataset(\n",
        "        images_dir=train_img_dir,\n",
        "        masks_dir=train_mask_dir,\n",
        "        transform=refuge_train_transform,\n",
        "        output_image_size=image_size,\n",
        "    )\n",
        "\n",
        "    refuge_val_dataset = REFUGEDataset(\n",
        "        images_dir=val_img_dir,\n",
        "        masks_dir=val_mask_dir,\n",
        "        transform=refuge_eval_transform,\n",
        "        output_image_size=image_size,\n",
        "    )\n",
        "\n",
        "    refuge_test_dataset = REFUGEDataset(\n",
        "        images_dir=test_img_dir,\n",
        "        masks_dir=test_mask_dir,\n",
        "        transform=refuge_eval_transform,\n",
        "        output_image_size=image_size,\n",
        "    )\n",
        "\n",
        "    #  DataLoaders\n",
        "    refuge_train_loader = DataLoader(\n",
        "        refuge_train_dataset,\n",
        "        batch_size=batch_size_train,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "    )\n",
        "    refuge_val_loader = DataLoader(\n",
        "        refuge_val_dataset,\n",
        "        batch_size=batch_size_val,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "    )\n",
        "    refuge_test_loader = DataLoader(\n",
        "        refuge_test_dataset,\n",
        "        batch_size=batch_size_test,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "    )\n",
        "\n",
        "    return refuge_train_loader, refuge_val_loader, refuge_test_loader, mean_refuge, std_refuge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e73sTP2jcunH",
        "outputId": "671208e2-a740-4316-968e-e3163feea1f1"
      },
      "outputs": [],
      "source": [
        "\n",
        "refuge_base_dir = \"/content/drive/MyDrive/Colab Notebooks/Traitement-image/data/REFUGE\"\n",
        "\n",
        "image_size_refuge = (512, 512)\n",
        "\n",
        "(\n",
        "    refuge_train_loader,\n",
        "    refuge_val_loader,\n",
        "    refuge_test_loader,\n",
        "    mean_refuge,\n",
        "    std_refuge,\n",
        ") = setup_refuge_dataloaders(\n",
        "    base_dir=refuge_base_dir,\n",
        "    image_size=image_size_refuge,\n",
        "    batch_size_train=2,\n",
        "    batch_size_val=2,\n",
        "    batch_size_test=1,\n",
        "    num_workers=2,\n",
        "    apply_polar=True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "srUuWKE6cqs0",
        "outputId": "f7f5bf0a-d87f-4c45-849d-da3cfb43ba7c"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Modèle REFUGE (disc + cup)\n",
        "# -----------------------------\n",
        "refuge_model = STDNetFullModel(\n",
        "    img_channels=3,\n",
        "    texture_channels=3,\n",
        "    hidden_dim=64,\n",
        "    img_size=image_size_refuge[0],\n",
        "    num_classes=2\n",
        ").to(device)\n",
        "\n",
        "optimizer_refuge = torch.optim.Adam(refuge_model.parameters(), lr=1e-3)\n",
        "bce_criterion_refuge = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
        "\n",
        "EPOCHS_REFUGE = 80\n",
        "history_refuge = {\"train_loss\": [], \"val_loss\": [], \"train_seg\": [], \"val_seg\": [],\n",
        "                  \"train_T\": [], \"val_T\": [], \"train_S\": [], \"val_S\": []}\n",
        "\n",
        "best_val_loss = float(\"inf\")\n",
        "best_epoch = -1\n",
        "best_model_path_refuge = \"stdnet_refuge_best.pth\"\n",
        "\n",
        "# Scheduler pour baisser le LR si la val stagne\n",
        "scheduler_refuge = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer_refuge,\n",
        "    mode=\"min\",\n",
        "    factor=0.5,\n",
        "    patience=10,\n",
        ")\n",
        "\n",
        "for epoch in range(1, EPOCHS_REFUGE + 1):\n",
        "\n",
        "    train_stats = train_one_epoch(\n",
        "        refuge_model,\n",
        "        refuge_train_loader,\n",
        "        optimizer_refuge,\n",
        "        device,\n",
        "        bce_criterion_refuge,\n",
        "    )\n",
        "\n",
        "    val_stats = validate_one_epoch(\n",
        "        refuge_model,\n",
        "        refuge_val_loader,\n",
        "        device,\n",
        "        bce_criterion_refuge,\n",
        "    )\n",
        "\n",
        "\n",
        "    scheduler_refuge.step(val_stats[\"loss\"])\n",
        "\n",
        "\n",
        "    log_epoch_stats(\n",
        "        history=history_refuge,\n",
        "        train_stats=train_stats,\n",
        "        val_stats=val_stats,\n",
        "        epoch=epoch,\n",
        "        total_epochs=EPOCHS_REFUGE,\n",
        "        tag=\"REFUGE\"\n",
        "    )\n",
        "\n",
        "\n",
        "    if val_stats[\"loss\"] < best_val_loss:\n",
        "        best_val_loss = val_stats[\"loss\"]\n",
        "        best_epoch = epoch\n",
        "        torch.save(refuge_model.state_dict(), best_model_path_refuge)\n",
        "        print(f\"==> Nouveau meilleur modèle REFUGE (epoch {epoch}, val_loss={best_val_loss:.4f})\")\n",
        "\n",
        "\n",
        "    show_origa_val_sample(\n",
        "        model=refuge_model,\n",
        "        val_loader=refuge_val_loader,\n",
        "        device=device,\n",
        "        title_prefix=f\"REFUGE - epoch {epoch}\",\n",
        "        threshold=0.5,\n",
        "    )\n",
        "\n",
        "\n",
        "best_refuge_model = STDNetFullModel(\n",
        "    img_channels=3,\n",
        "    texture_channels=3,\n",
        "    hidden_dim=64,\n",
        "    img_size=image_size_refuge[0],\n",
        "    num_classes=2\n",
        ").to(device)\n",
        "best_refuge_model.load_state_dict(torch.load(best_model_path_refuge, map_location=device))\n",
        "\n",
        "\n",
        "oe_refuge_test = compute_oe_origa(best_refuge_model, refuge_test_loader, device, desc=\"OE REFUGE TEST\")\n",
        "\n",
        "print(\"\\n===== OE REFUGE (disc + cup) =====\")\n",
        "print(f\"Test : OE_disc={oe_refuge_test['disc']:.4f}, OEcup={oe_refuge_test['cup']:.4f}, OEtotal={oe_refuge_test['total']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "-2P7rjE1D0lt",
        "outputId": "87f964e3-6581-4d50-b36f-ef6ff3cb23cb"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# ----- COURBES ORIGA -----\n",
        "epochs_origa = np.arange(1, len(history_refuge[\"train_loss\"]) + 1)\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "#Loss totale\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(epochs_origa, history_refuge[\"train_loss\"], label=\"train\")\n",
        "plt.plot(epochs_origa, history_refuge[\"val_loss\"],   label=\"val\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Total loss\")\n",
        "plt.title(\"ORIGA - Total loss\")\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.3)\n",
        "\n",
        "#  Loss segmentation\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(epochs_origa, history_refuge[\"train_seg\"], label=\"train\")\n",
        "plt.plot(epochs_origa, history_refuge[\"val_seg\"],   label=\"val\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Segmentation loss\")\n",
        "plt.title(\"ORIGA - Segmentation\")\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.3)\n",
        "\n",
        "# Loss texture\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot(epochs_origa, history_refuge[\"train_T\"], label=\"train\")\n",
        "plt.plot(epochs_origa, history_refuge[\"val_T\"],   label=\"val\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Texture loss (Lt)\")\n",
        "plt.title(\"ORIGA - Texture\")\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.3)\n",
        "\n",
        "# Loss structure\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.plot(epochs_origa, history_refuge[\"train_S\"], label=\"train\")\n",
        "plt.plot(epochs_origa, history_refuge[\"val_S\"],   label=\"val\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Structure loss (Ls)\")\n",
        "plt.title(\"ORIGA - Structure\")\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
